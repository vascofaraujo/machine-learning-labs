{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6484c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from sklearn.metrics import balanced_accuracy_score as BACC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import eval_scores as eval\n",
    "import cv2 as cv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5ee13ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(xtrain, ytrain):\n",
    "    print(xtrain.shape, ytrain.shape)\n",
    "    #(6470, 50, 50) (6470,)\n",
    "    #(50, 50) ()\n",
    "    xtrain_len = len(xtrain)\n",
    "    \n",
    "    aug_xtrain = np.zeros((xtrain_len*2, 50, 50))\n",
    "    aug_ytrain = np.zeros((xtrain_len*2))\n",
    "    \n",
    "    aug_xtrain[0:xtrain_len, :, :] = xtrain\n",
    "    aug_ytrain[0:xtrain_len] = ytrain\n",
    "    \n",
    "    for idx in range(xtrain_len):\n",
    "        image = xtrain[idx,:,:]\n",
    "        label = ytrain[idx]\n",
    "                \n",
    "        angle = int(random.uniform(-90, 90))\n",
    "        h, w = image.shape[0], image.shape[1]\n",
    "        M = cv.getRotationMatrix2D((int(w/2), int(h/2)), angle, 1)\n",
    "        rotated = cv.warpAffine(image, M, (w, h))\n",
    "        \n",
    "        flipped = cv.flip(rotated, 1)\n",
    "\n",
    "\n",
    "        aug_xtrain[xtrain_len+idx] = flipped\n",
    "        aug_ytrain[xtrain_len+idx] = label\n",
    "        \n",
    "        \n",
    "    return aug_xtrain, aug_ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cc81ba2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6470, 50, 50) (6470,)\n"
     ]
    }
   ],
   "source": [
    "xtrain = np.load(\"Xtrain_Classification_Part1.npy\")\n",
    "ytrain = np.load(\"Ytrain_Classification_Part1.npy\")\n",
    "xtrain_len = len(xtrain)\n",
    "ytrain_len = len(ytrain)\n",
    "\n",
    "#Reshape Images\n",
    "xtrain = xtrain.reshape((xtrain_len,50,50))\n",
    "mean = xtrain.mean(axis=(0, 1, 2)) \n",
    "std = xtrain.std(axis=(0, 1, 2))\n",
    "\n",
    "xtrain = (xtrain - mean)/std  \n",
    "\n",
    "xtrain, ytrain = augment_data(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cc4d8a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xtrain[6479,:,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "dab60c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        xtrain = np.load(\"Xtrain_Classification_Part1.npy\")\n",
    "        ytrain = np.load(\"Ytrain_Classification_Part1.npy\")\n",
    "        xtest = np.load(\"Xtest_Classification_Part1.npy\")\n",
    "\n",
    "\n",
    "        xtrain_len = len(xtrain)\n",
    "        ytrain_len = len(ytrain)\n",
    "        xtest_len = len(xtest)\n",
    "\n",
    "        #Reshape Images\n",
    "        xtrain = xtrain.reshape((xtrain_len,50,50))\n",
    "        mean = xtrain.mean(axis=(0, 1, 2)) \n",
    "        std = xtrain.std(axis=(0, 1, 2))\n",
    "\n",
    "        xtrain = (xtrain - mean)/std  \n",
    "        \n",
    "        xtrain, ytrain = augment_data(xtrain, ytrain)\n",
    "        \n",
    "        new_xtrain_len = len(xtrain)\n",
    "        new_ytrain_len = len(ytrain)\n",
    "        \n",
    "        self.xtrain = xtrain.reshape((new_xtrain_len,1,50,50))\n",
    "        self.xtest = xtest.reshape((xtest_len,1,50,50))\n",
    "\n",
    "        self.ytrain = ytrain.reshape(new_ytrain_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xtest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    " \n",
    "        \n",
    "        image = self.xtest[idx, :, :, :]\n",
    "        label = self.ytrain[idx]\n",
    "        \n",
    "\n",
    "        return image, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bff7d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        xtest = np.load(\"Xtest_Classification_Part1.npy\")\n",
    "\n",
    "\n",
    "        xtest_len = len(xtest)\n",
    "\n",
    "\n",
    "        self.xtest = xtest.reshape((xtest_len,1,50,50))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xtest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        \n",
    "        image = self.xtest[idx, :, :, :]\n",
    "        \n",
    "\n",
    "        return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c6ff07e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ClassifyNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3,\n",
    "                              kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10,\n",
    "                              kernel_size=5, stride=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(in_features=2500, out_features=400)\n",
    "        self.fc2 = nn.Linear(in_features=1587, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        #x = self.conv2(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.fc1(x)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "301b424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size' : 64,\n",
    "    'lr' : 0.001,\n",
    "    'momentum' : 0.9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ca190a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    \n",
    "    num_epochs = 400\n",
    "    best_score = 1.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        for idx, data in enumerate(train_loader):\n",
    "            image, label = data[0].float().to(device), data[1].float()\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            classify_output = model(image)\n",
    "            \n",
    "            \n",
    "            y_true = torch.reshape(label.cpu(), (-1,))\n",
    "            y_pred = torch.reshape(classify_output.cpu(), (-1,))\n",
    "            \n",
    "            loss = criterion(y_pred, y_true)\n",
    "\n",
    "            #print(loss)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print for mini batches\n",
    "            running_loss += loss.item()\n",
    "            if idx % 50 == 49:  # every 50 mini batches\n",
    "                print('[Epoch %d, %5d Mini Batches] loss: %.3f' %\n",
    "                      (epoch + 1, idx + 1, running_loss/50))\n",
    "                running_loss = 0.0\n",
    "               \n",
    "        model.eval()\n",
    "        test_score = 0.0\n",
    "        \n",
    "         \n",
    "        with torch.no_grad():\n",
    "            for idx, data in enumerate(test_loader):\n",
    "                image, label = data[0].float().to(device), data[1].float()\n",
    "\n",
    "                classify_output = model(image)\n",
    "\n",
    "                y_true = torch.reshape(label.cpu(), (-1,))\n",
    "                y_pred = torch.reshape(classify_output.cpu(), (-1,))\n",
    "\n",
    "                test_loss = criterion(y_pred, y_true)\n",
    "\n",
    "                test_score += test_loss\n",
    "\n",
    "            test_score /= len(test_loader)\n",
    "\n",
    "            print(test_score)\n",
    "\n",
    "            if test_score < best_score:\n",
    "                torch.save(model, \"ClassifyNet.pth\")\n",
    "                best_score = test_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bc1d7acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6470, 50, 50) (6470,)\n"
     ]
    }
   ],
   "source": [
    "data_set = ImageDataset()\n",
    "\n",
    "train_set, test_set = train_test_split(data_set, test_size=0.2, random_state=1, shuffle=True)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=config['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=config['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bf166f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4a11251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device= torch.device('cpu')\n",
    "model = ClassifyNet().to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=config['momentum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "862bd8da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1,    50 Mini Batches] loss: 0.644\n",
      "[Epoch 1,   100 Mini Batches] loss: 0.600\n",
      "[Epoch 1,   150 Mini Batches] loss: 0.580\n",
      "tensor(0.5505)\n",
      "[Epoch 2,    50 Mini Batches] loss: 0.563\n",
      "[Epoch 2,   100 Mini Batches] loss: 0.555\n",
      "[Epoch 2,   150 Mini Batches] loss: 0.553\n",
      "tensor(0.5226)\n",
      "[Epoch 3,    50 Mini Batches] loss: 0.544\n",
      "[Epoch 3,   100 Mini Batches] loss: 0.525\n",
      "[Epoch 3,   150 Mini Batches] loss: 0.538\n",
      "tensor(0.4989)\n",
      "[Epoch 4,    50 Mini Batches] loss: 0.511\n",
      "[Epoch 4,   100 Mini Batches] loss: 0.521\n",
      "[Epoch 4,   150 Mini Batches] loss: 0.517\n",
      "tensor(0.4869)\n",
      "[Epoch 5,    50 Mini Batches] loss: 0.510\n",
      "[Epoch 5,   100 Mini Batches] loss: 0.508\n",
      "[Epoch 5,   150 Mini Batches] loss: 0.486\n",
      "tensor(0.4740)\n",
      "[Epoch 6,    50 Mini Batches] loss: 0.492\n",
      "[Epoch 6,   100 Mini Batches] loss: 0.499\n",
      "[Epoch 6,   150 Mini Batches] loss: 0.489\n",
      "tensor(0.4725)\n",
      "[Epoch 7,    50 Mini Batches] loss: 0.492\n",
      "[Epoch 7,   100 Mini Batches] loss: 0.489\n",
      "[Epoch 7,   150 Mini Batches] loss: 0.487\n",
      "tensor(0.4677)\n",
      "[Epoch 8,    50 Mini Batches] loss: 0.491\n",
      "[Epoch 8,   100 Mini Batches] loss: 0.480\n",
      "[Epoch 8,   150 Mini Batches] loss: 0.473\n",
      "tensor(0.4581)\n",
      "[Epoch 9,    50 Mini Batches] loss: 0.476\n",
      "[Epoch 9,   100 Mini Batches] loss: 0.466\n",
      "[Epoch 9,   150 Mini Batches] loss: 0.497\n",
      "tensor(0.4568)\n",
      "[Epoch 10,    50 Mini Batches] loss: 0.473\n",
      "[Epoch 10,   100 Mini Batches] loss: 0.483\n",
      "[Epoch 10,   150 Mini Batches] loss: 0.467\n",
      "tensor(0.4474)\n",
      "[Epoch 11,    50 Mini Batches] loss: 0.472\n",
      "[Epoch 11,   100 Mini Batches] loss: 0.466\n",
      "[Epoch 11,   150 Mini Batches] loss: 0.468\n",
      "tensor(0.4591)\n",
      "[Epoch 12,    50 Mini Batches] loss: 0.471\n",
      "[Epoch 12,   100 Mini Batches] loss: 0.473\n",
      "[Epoch 12,   150 Mini Batches] loss: 0.455\n",
      "tensor(0.4452)\n",
      "[Epoch 13,    50 Mini Batches] loss: 0.458\n",
      "[Epoch 13,   100 Mini Batches] loss: 0.460\n",
      "[Epoch 13,   150 Mini Batches] loss: 0.460\n",
      "tensor(0.4432)\n",
      "[Epoch 14,    50 Mini Batches] loss: 0.460\n",
      "[Epoch 14,   100 Mini Batches] loss: 0.465\n",
      "[Epoch 14,   150 Mini Batches] loss: 0.461\n",
      "tensor(0.4391)\n",
      "[Epoch 15,    50 Mini Batches] loss: 0.464\n",
      "[Epoch 15,   100 Mini Batches] loss: 0.451\n",
      "[Epoch 15,   150 Mini Batches] loss: 0.446\n",
      "tensor(0.4363)\n",
      "[Epoch 16,    50 Mini Batches] loss: 0.458\n",
      "[Epoch 16,   100 Mini Batches] loss: 0.457\n",
      "[Epoch 16,   150 Mini Batches] loss: 0.458\n",
      "tensor(0.4392)\n",
      "[Epoch 17,    50 Mini Batches] loss: 0.454\n",
      "[Epoch 17,   100 Mini Batches] loss: 0.457\n",
      "[Epoch 17,   150 Mini Batches] loss: 0.452\n",
      "tensor(0.4402)\n",
      "[Epoch 18,    50 Mini Batches] loss: 0.443\n",
      "[Epoch 18,   100 Mini Batches] loss: 0.446\n",
      "[Epoch 18,   150 Mini Batches] loss: 0.461\n",
      "tensor(0.4319)\n",
      "[Epoch 19,    50 Mini Batches] loss: 0.443\n",
      "[Epoch 19,   100 Mini Batches] loss: 0.448\n",
      "[Epoch 19,   150 Mini Batches] loss: 0.451\n",
      "tensor(0.4307)\n",
      "[Epoch 20,    50 Mini Batches] loss: 0.456\n",
      "[Epoch 20,   100 Mini Batches] loss: 0.442\n",
      "[Epoch 20,   150 Mini Batches] loss: 0.447\n",
      "tensor(0.4314)\n",
      "[Epoch 21,    50 Mini Batches] loss: 0.446\n",
      "[Epoch 21,   100 Mini Batches] loss: 0.439\n",
      "[Epoch 21,   150 Mini Batches] loss: 0.451\n",
      "tensor(0.4263)\n",
      "[Epoch 22,    50 Mini Batches] loss: 0.427\n",
      "[Epoch 22,   100 Mini Batches] loss: 0.455\n",
      "[Epoch 22,   150 Mini Batches] loss: 0.441\n",
      "tensor(0.4250)\n",
      "[Epoch 23,    50 Mini Batches] loss: 0.421\n",
      "[Epoch 23,   100 Mini Batches] loss: 0.436\n",
      "[Epoch 23,   150 Mini Batches] loss: 0.457\n",
      "tensor(0.4351)\n",
      "[Epoch 24,    50 Mini Batches] loss: 0.437\n",
      "[Epoch 24,   100 Mini Batches] loss: 0.452\n",
      "[Epoch 24,   150 Mini Batches] loss: 0.437\n",
      "tensor(0.4251)\n",
      "[Epoch 25,    50 Mini Batches] loss: 0.433\n",
      "[Epoch 25,   100 Mini Batches] loss: 0.448\n",
      "[Epoch 25,   150 Mini Batches] loss: 0.433\n",
      "tensor(0.4255)\n",
      "[Epoch 26,    50 Mini Batches] loss: 0.442\n",
      "[Epoch 26,   100 Mini Batches] loss: 0.449\n",
      "[Epoch 26,   150 Mini Batches] loss: 0.431\n",
      "tensor(0.4253)\n",
      "[Epoch 27,    50 Mini Batches] loss: 0.452\n",
      "[Epoch 27,   100 Mini Batches] loss: 0.435\n",
      "[Epoch 27,   150 Mini Batches] loss: 0.436\n",
      "tensor(0.4242)\n",
      "[Epoch 28,    50 Mini Batches] loss: 0.426\n",
      "[Epoch 28,   100 Mini Batches] loss: 0.434\n",
      "[Epoch 28,   150 Mini Batches] loss: 0.439\n",
      "tensor(0.4197)\n",
      "[Epoch 29,    50 Mini Batches] loss: 0.435\n",
      "[Epoch 29,   100 Mini Batches] loss: 0.439\n",
      "[Epoch 29,   150 Mini Batches] loss: 0.430\n",
      "tensor(0.4201)\n",
      "[Epoch 30,    50 Mini Batches] loss: 0.427\n",
      "[Epoch 30,   100 Mini Batches] loss: 0.444\n",
      "[Epoch 30,   150 Mini Batches] loss: 0.440\n",
      "tensor(0.4236)\n",
      "[Epoch 31,    50 Mini Batches] loss: 0.436\n",
      "[Epoch 31,   100 Mini Batches] loss: 0.431\n",
      "[Epoch 31,   150 Mini Batches] loss: 0.437\n",
      "tensor(0.4270)\n",
      "[Epoch 32,    50 Mini Batches] loss: 0.430\n",
      "[Epoch 32,   100 Mini Batches] loss: 0.429\n",
      "[Epoch 32,   150 Mini Batches] loss: 0.439\n",
      "tensor(0.4249)\n",
      "[Epoch 33,    50 Mini Batches] loss: 0.432\n",
      "[Epoch 33,   100 Mini Batches] loss: 0.432\n",
      "[Epoch 33,   150 Mini Batches] loss: 0.434\n",
      "tensor(0.4194)\n",
      "[Epoch 34,    50 Mini Batches] loss: 0.447\n",
      "[Epoch 34,   100 Mini Batches] loss: 0.423\n",
      "[Epoch 34,   150 Mini Batches] loss: 0.423\n",
      "tensor(0.4166)\n",
      "[Epoch 35,    50 Mini Batches] loss: 0.432\n",
      "[Epoch 35,   100 Mini Batches] loss: 0.426\n",
      "[Epoch 35,   150 Mini Batches] loss: 0.423\n",
      "tensor(0.4191)\n",
      "[Epoch 36,    50 Mini Batches] loss: 0.435\n",
      "[Epoch 36,   100 Mini Batches] loss: 0.431\n",
      "[Epoch 36,   150 Mini Batches] loss: 0.431\n",
      "tensor(0.4191)\n",
      "[Epoch 37,    50 Mini Batches] loss: 0.425\n",
      "[Epoch 37,   100 Mini Batches] loss: 0.425\n",
      "[Epoch 37,   150 Mini Batches] loss: 0.427\n",
      "tensor(0.4174)\n",
      "[Epoch 38,    50 Mini Batches] loss: 0.431\n",
      "[Epoch 38,   100 Mini Batches] loss: 0.423\n",
      "[Epoch 38,   150 Mini Batches] loss: 0.441\n",
      "tensor(0.4191)\n",
      "[Epoch 39,    50 Mini Batches] loss: 0.428\n",
      "[Epoch 39,   100 Mini Batches] loss: 0.418\n",
      "[Epoch 39,   150 Mini Batches] loss: 0.448\n",
      "tensor(0.4197)\n",
      "[Epoch 40,    50 Mini Batches] loss: 0.435\n",
      "[Epoch 40,   100 Mini Batches] loss: 0.425\n",
      "[Epoch 40,   150 Mini Batches] loss: 0.418\n",
      "tensor(0.4156)\n",
      "[Epoch 41,    50 Mini Batches] loss: 0.437\n",
      "[Epoch 41,   100 Mini Batches] loss: 0.428\n",
      "[Epoch 41,   150 Mini Batches] loss: 0.419\n",
      "tensor(0.4194)\n",
      "[Epoch 42,    50 Mini Batches] loss: 0.420\n",
      "[Epoch 42,   100 Mini Batches] loss: 0.424\n",
      "[Epoch 42,   150 Mini Batches] loss: 0.433\n",
      "tensor(0.4167)\n",
      "[Epoch 43,    50 Mini Batches] loss: 0.403\n",
      "[Epoch 43,   100 Mini Batches] loss: 0.437\n",
      "[Epoch 43,   150 Mini Batches] loss: 0.431\n",
      "tensor(0.4212)\n",
      "[Epoch 44,    50 Mini Batches] loss: 0.444\n",
      "[Epoch 44,   100 Mini Batches] loss: 0.429\n",
      "[Epoch 44,   150 Mini Batches] loss: 0.433\n",
      "tensor(0.4197)\n",
      "[Epoch 45,    50 Mini Batches] loss: 0.419\n",
      "[Epoch 45,   100 Mini Batches] loss: 0.453\n",
      "[Epoch 45,   150 Mini Batches] loss: 0.421\n",
      "tensor(0.4180)\n",
      "[Epoch 46,    50 Mini Batches] loss: 0.418\n",
      "[Epoch 46,   100 Mini Batches] loss: 0.429\n",
      "[Epoch 46,   150 Mini Batches] loss: 0.418\n",
      "tensor(0.4154)\n",
      "[Epoch 47,    50 Mini Batches] loss: 0.422\n",
      "[Epoch 47,   100 Mini Batches] loss: 0.423\n",
      "[Epoch 47,   150 Mini Batches] loss: 0.426\n",
      "tensor(0.4166)\n",
      "[Epoch 48,    50 Mini Batches] loss: 0.421\n",
      "[Epoch 48,   100 Mini Batches] loss: 0.414\n",
      "[Epoch 48,   150 Mini Batches] loss: 0.424\n",
      "tensor(0.4130)\n",
      "[Epoch 49,    50 Mini Batches] loss: 0.420\n",
      "[Epoch 49,   100 Mini Batches] loss: 0.414\n",
      "[Epoch 49,   150 Mini Batches] loss: 0.427\n",
      "tensor(0.4162)\n",
      "[Epoch 50,    50 Mini Batches] loss: 0.434\n",
      "[Epoch 50,   100 Mini Batches] loss: 0.416\n",
      "[Epoch 50,   150 Mini Batches] loss: 0.420\n",
      "tensor(0.4202)\n",
      "[Epoch 51,    50 Mini Batches] loss: 0.426\n",
      "[Epoch 51,   100 Mini Batches] loss: 0.423\n",
      "[Epoch 51,   150 Mini Batches] loss: 0.427\n",
      "tensor(0.4173)\n",
      "[Epoch 52,    50 Mini Batches] loss: 0.423\n",
      "[Epoch 52,   100 Mini Batches] loss: 0.420\n",
      "[Epoch 52,   150 Mini Batches] loss: 0.423\n",
      "tensor(0.4164)\n",
      "[Epoch 53,    50 Mini Batches] loss: 0.425\n",
      "[Epoch 53,   100 Mini Batches] loss: 0.418\n",
      "[Epoch 53,   150 Mini Batches] loss: 0.424\n",
      "tensor(0.4168)\n",
      "[Epoch 54,    50 Mini Batches] loss: 0.416\n",
      "[Epoch 54,   100 Mini Batches] loss: 0.423\n",
      "[Epoch 54,   150 Mini Batches] loss: 0.422\n",
      "tensor(0.4122)\n",
      "[Epoch 55,    50 Mini Batches] loss: 0.439\n",
      "[Epoch 55,   100 Mini Batches] loss: 0.405\n",
      "[Epoch 55,   150 Mini Batches] loss: 0.425\n",
      "tensor(0.4126)\n",
      "[Epoch 56,    50 Mini Batches] loss: 0.416\n",
      "[Epoch 56,   100 Mini Batches] loss: 0.411\n",
      "[Epoch 56,   150 Mini Batches] loss: 0.428\n",
      "tensor(0.4158)\n",
      "[Epoch 57,    50 Mini Batches] loss: 0.433\n",
      "[Epoch 57,   100 Mini Batches] loss: 0.430\n",
      "[Epoch 57,   150 Mini Batches] loss: 0.403\n",
      "tensor(0.4137)\n",
      "[Epoch 58,    50 Mini Batches] loss: 0.416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 58,   100 Mini Batches] loss: 0.419\n",
      "[Epoch 58,   150 Mini Batches] loss: 0.422\n",
      "tensor(0.4124)\n",
      "[Epoch 59,    50 Mini Batches] loss: 0.401\n",
      "[Epoch 59,   100 Mini Batches] loss: 0.423\n",
      "[Epoch 59,   150 Mini Batches] loss: 0.421\n",
      "tensor(0.4152)\n",
      "[Epoch 60,    50 Mini Batches] loss: 0.420\n",
      "[Epoch 60,   100 Mini Batches] loss: 0.420\n",
      "[Epoch 60,   150 Mini Batches] loss: 0.411\n",
      "tensor(0.4096)\n",
      "[Epoch 61,    50 Mini Batches] loss: 0.413\n",
      "[Epoch 61,   100 Mini Batches] loss: 0.418\n",
      "[Epoch 61,   150 Mini Batches] loss: 0.419\n",
      "tensor(0.4124)\n",
      "[Epoch 62,    50 Mini Batches] loss: 0.406\n",
      "[Epoch 62,   100 Mini Batches] loss: 0.421\n",
      "[Epoch 62,   150 Mini Batches] loss: 0.429\n",
      "tensor(0.4180)\n",
      "[Epoch 63,    50 Mini Batches] loss: 0.419\n",
      "[Epoch 63,   100 Mini Batches] loss: 0.414\n",
      "[Epoch 63,   150 Mini Batches] loss: 0.414\n",
      "tensor(0.4091)\n",
      "[Epoch 64,    50 Mini Batches] loss: 0.410\n",
      "[Epoch 64,   100 Mini Batches] loss: 0.425\n",
      "[Epoch 64,   150 Mini Batches] loss: 0.415\n",
      "tensor(0.4166)\n",
      "[Epoch 65,    50 Mini Batches] loss: 0.417\n",
      "[Epoch 65,   100 Mini Batches] loss: 0.410\n",
      "[Epoch 65,   150 Mini Batches] loss: 0.423\n",
      "tensor(0.4137)\n",
      "[Epoch 66,    50 Mini Batches] loss: 0.418\n",
      "[Epoch 66,   100 Mini Batches] loss: 0.406\n",
      "[Epoch 66,   150 Mini Batches] loss: 0.423\n",
      "tensor(0.4111)\n",
      "[Epoch 67,    50 Mini Batches] loss: 0.421\n",
      "[Epoch 67,   100 Mini Batches] loss: 0.409\n",
      "[Epoch 67,   150 Mini Batches] loss: 0.408\n",
      "tensor(0.4125)\n",
      "[Epoch 68,    50 Mini Batches] loss: 0.411\n",
      "[Epoch 68,   100 Mini Batches] loss: 0.417\n",
      "[Epoch 68,   150 Mini Batches] loss: 0.413\n",
      "tensor(0.4137)\n",
      "[Epoch 69,    50 Mini Batches] loss: 0.414\n",
      "[Epoch 69,   100 Mini Batches] loss: 0.400\n",
      "[Epoch 69,   150 Mini Batches] loss: 0.415\n",
      "tensor(0.4097)\n",
      "[Epoch 70,    50 Mini Batches] loss: 0.422\n",
      "[Epoch 70,   100 Mini Batches] loss: 0.406\n",
      "[Epoch 70,   150 Mini Batches] loss: 0.415\n",
      "tensor(0.4115)\n",
      "[Epoch 71,    50 Mini Batches] loss: 0.416\n",
      "[Epoch 71,   100 Mini Batches] loss: 0.416\n",
      "[Epoch 71,   150 Mini Batches] loss: 0.417\n",
      "tensor(0.4077)\n",
      "[Epoch 72,    50 Mini Batches] loss: 0.419\n",
      "[Epoch 72,   100 Mini Batches] loss: 0.403\n",
      "[Epoch 72,   150 Mini Batches] loss: 0.424\n",
      "tensor(0.4113)\n",
      "[Epoch 73,    50 Mini Batches] loss: 0.425\n",
      "[Epoch 73,   100 Mini Batches] loss: 0.392\n",
      "[Epoch 73,   150 Mini Batches] loss: 0.433\n",
      "tensor(0.4083)\n",
      "[Epoch 74,    50 Mini Batches] loss: 0.411\n",
      "[Epoch 74,   100 Mini Batches] loss: 0.397\n",
      "[Epoch 74,   150 Mini Batches] loss: 0.424\n",
      "tensor(0.4093)\n",
      "[Epoch 75,    50 Mini Batches] loss: 0.404\n",
      "[Epoch 75,   100 Mini Batches] loss: 0.414\n",
      "[Epoch 75,   150 Mini Batches] loss: 0.424\n",
      "tensor(0.4108)\n",
      "[Epoch 76,    50 Mini Batches] loss: 0.416\n",
      "[Epoch 76,   100 Mini Batches] loss: 0.429\n",
      "[Epoch 76,   150 Mini Batches] loss: 0.418\n",
      "tensor(0.4097)\n",
      "[Epoch 77,    50 Mini Batches] loss: 0.418\n",
      "[Epoch 77,   100 Mini Batches] loss: 0.417\n",
      "[Epoch 77,   150 Mini Batches] loss: 0.395\n",
      "tensor(0.4110)\n",
      "[Epoch 78,    50 Mini Batches] loss: 0.419\n",
      "[Epoch 78,   100 Mini Batches] loss: 0.420\n",
      "[Epoch 78,   150 Mini Batches] loss: 0.402\n",
      "tensor(0.4103)\n",
      "[Epoch 79,    50 Mini Batches] loss: 0.403\n",
      "[Epoch 79,   100 Mini Batches] loss: 0.423\n",
      "[Epoch 79,   150 Mini Batches] loss: 0.410\n",
      "tensor(0.4113)\n",
      "[Epoch 80,    50 Mini Batches] loss: 0.413\n",
      "[Epoch 80,   100 Mini Batches] loss: 0.401\n",
      "[Epoch 80,   150 Mini Batches] loss: 0.425\n",
      "tensor(0.4131)\n",
      "[Epoch 81,    50 Mini Batches] loss: 0.412\n",
      "[Epoch 81,   100 Mini Batches] loss: 0.408\n",
      "[Epoch 81,   150 Mini Batches] loss: 0.413\n",
      "tensor(0.4109)\n",
      "[Epoch 82,    50 Mini Batches] loss: 0.403\n",
      "[Epoch 82,   100 Mini Batches] loss: 0.414\n",
      "[Epoch 82,   150 Mini Batches] loss: 0.414\n",
      "tensor(0.4118)\n",
      "[Epoch 83,    50 Mini Batches] loss: 0.418\n",
      "[Epoch 83,   100 Mini Batches] loss: 0.414\n",
      "[Epoch 83,   150 Mini Batches] loss: 0.401\n",
      "tensor(0.4099)\n",
      "[Epoch 84,    50 Mini Batches] loss: 0.418\n",
      "[Epoch 84,   100 Mini Batches] loss: 0.403\n",
      "[Epoch 84,   150 Mini Batches] loss: 0.408\n",
      "tensor(0.4091)\n",
      "[Epoch 85,    50 Mini Batches] loss: 0.395\n",
      "[Epoch 85,   100 Mini Batches] loss: 0.421\n",
      "[Epoch 85,   150 Mini Batches] loss: 0.425\n",
      "tensor(0.4106)\n",
      "[Epoch 86,    50 Mini Batches] loss: 0.408\n",
      "[Epoch 86,   100 Mini Batches] loss: 0.409\n",
      "[Epoch 86,   150 Mini Batches] loss: 0.418\n",
      "tensor(0.4132)\n",
      "[Epoch 87,    50 Mini Batches] loss: 0.407\n",
      "[Epoch 87,   100 Mini Batches] loss: 0.426\n",
      "[Epoch 87,   150 Mini Batches] loss: 0.401\n",
      "tensor(0.4096)\n",
      "[Epoch 88,    50 Mini Batches] loss: 0.412\n",
      "[Epoch 88,   100 Mini Batches] loss: 0.413\n",
      "[Epoch 88,   150 Mini Batches] loss: 0.411\n",
      "tensor(0.4082)\n",
      "[Epoch 89,    50 Mini Batches] loss: 0.403\n",
      "[Epoch 89,   100 Mini Batches] loss: 0.398\n",
      "[Epoch 89,   150 Mini Batches] loss: 0.422\n",
      "tensor(0.4124)\n",
      "[Epoch 90,    50 Mini Batches] loss: 0.423\n",
      "[Epoch 90,   100 Mini Batches] loss: 0.406\n",
      "[Epoch 90,   150 Mini Batches] loss: 0.400\n",
      "tensor(0.4114)\n",
      "[Epoch 91,    50 Mini Batches] loss: 0.425\n",
      "[Epoch 91,   100 Mini Batches] loss: 0.398\n",
      "[Epoch 91,   150 Mini Batches] loss: 0.409\n",
      "tensor(0.4143)\n",
      "[Epoch 92,    50 Mini Batches] loss: 0.397\n",
      "[Epoch 92,   100 Mini Batches] loss: 0.413\n",
      "[Epoch 92,   150 Mini Batches] loss: 0.431\n",
      "tensor(0.4101)\n",
      "[Epoch 93,    50 Mini Batches] loss: 0.400\n",
      "[Epoch 93,   100 Mini Batches] loss: 0.398\n",
      "[Epoch 93,   150 Mini Batches] loss: 0.414\n",
      "tensor(0.4094)\n",
      "[Epoch 94,    50 Mini Batches] loss: 0.401\n",
      "[Epoch 94,   100 Mini Batches] loss: 0.397\n",
      "[Epoch 94,   150 Mini Batches] loss: 0.408\n",
      "tensor(0.4102)\n",
      "[Epoch 95,    50 Mini Batches] loss: 0.421\n",
      "[Epoch 95,   100 Mini Batches] loss: 0.404\n",
      "[Epoch 95,   150 Mini Batches] loss: 0.402\n",
      "tensor(0.4095)\n",
      "[Epoch 96,    50 Mini Batches] loss: 0.397\n",
      "[Epoch 96,   100 Mini Batches] loss: 0.411\n",
      "[Epoch 96,   150 Mini Batches] loss: 0.413\n",
      "tensor(0.4092)\n",
      "[Epoch 97,    50 Mini Batches] loss: 0.397\n",
      "[Epoch 97,   100 Mini Batches] loss: 0.427\n",
      "[Epoch 97,   150 Mini Batches] loss: 0.395\n",
      "tensor(0.4119)\n",
      "[Epoch 98,    50 Mini Batches] loss: 0.410\n",
      "[Epoch 98,   100 Mini Batches] loss: 0.393\n",
      "[Epoch 98,   150 Mini Batches] loss: 0.410\n",
      "tensor(0.4107)\n",
      "[Epoch 99,    50 Mini Batches] loss: 0.410\n",
      "[Epoch 99,   100 Mini Batches] loss: 0.408\n",
      "[Epoch 99,   150 Mini Batches] loss: 0.403\n",
      "tensor(0.4134)\n",
      "[Epoch 100,    50 Mini Batches] loss: 0.416\n",
      "[Epoch 100,   100 Mini Batches] loss: 0.410\n",
      "[Epoch 100,   150 Mini Batches] loss: 0.400\n",
      "tensor(0.4165)\n",
      "[Epoch 101,    50 Mini Batches] loss: 0.412\n",
      "[Epoch 101,   100 Mini Batches] loss: 0.407\n",
      "[Epoch 101,   150 Mini Batches] loss: 0.401\n",
      "tensor(0.4055)\n",
      "[Epoch 102,    50 Mini Batches] loss: 0.413\n",
      "[Epoch 102,   100 Mini Batches] loss: 0.394\n",
      "[Epoch 102,   150 Mini Batches] loss: 0.417\n",
      "tensor(0.4078)\n",
      "[Epoch 103,    50 Mini Batches] loss: 0.406\n",
      "[Epoch 103,   100 Mini Batches] loss: 0.411\n",
      "[Epoch 103,   150 Mini Batches] loss: 0.393\n",
      "tensor(0.4085)\n",
      "[Epoch 104,    50 Mini Batches] loss: 0.405\n",
      "[Epoch 104,   100 Mini Batches] loss: 0.404\n",
      "[Epoch 104,   150 Mini Batches] loss: 0.416\n",
      "tensor(0.4089)\n",
      "[Epoch 105,    50 Mini Batches] loss: 0.394\n",
      "[Epoch 105,   100 Mini Batches] loss: 0.409\n",
      "[Epoch 105,   150 Mini Batches] loss: 0.414\n",
      "tensor(0.4080)\n",
      "[Epoch 106,    50 Mini Batches] loss: 0.411\n",
      "[Epoch 106,   100 Mini Batches] loss: 0.422\n",
      "[Epoch 106,   150 Mini Batches] loss: 0.406\n",
      "tensor(0.4085)\n",
      "[Epoch 107,    50 Mini Batches] loss: 0.412\n",
      "[Epoch 107,   100 Mini Batches] loss: 0.417\n",
      "[Epoch 107,   150 Mini Batches] loss: 0.404\n",
      "tensor(0.4102)\n",
      "[Epoch 108,    50 Mini Batches] loss: 0.419\n",
      "[Epoch 108,   100 Mini Batches] loss: 0.416\n",
      "[Epoch 108,   150 Mini Batches] loss: 0.400\n",
      "tensor(0.4083)\n",
      "[Epoch 109,    50 Mini Batches] loss: 0.396\n",
      "[Epoch 109,   100 Mini Batches] loss: 0.396\n",
      "[Epoch 109,   150 Mini Batches] loss: 0.425\n",
      "tensor(0.4102)\n",
      "[Epoch 110,    50 Mini Batches] loss: 0.396\n",
      "[Epoch 110,   100 Mini Batches] loss: 0.411\n",
      "[Epoch 110,   150 Mini Batches] loss: 0.415\n",
      "tensor(0.4087)\n",
      "[Epoch 111,    50 Mini Batches] loss: 0.421\n",
      "[Epoch 111,   100 Mini Batches] loss: 0.391\n",
      "[Epoch 111,   150 Mini Batches] loss: 0.409\n",
      "tensor(0.4060)\n",
      "[Epoch 112,    50 Mini Batches] loss: 0.406\n",
      "[Epoch 112,   100 Mini Batches] loss: 0.380\n",
      "[Epoch 112,   150 Mini Batches] loss: 0.418\n",
      "tensor(0.4107)\n",
      "[Epoch 113,    50 Mini Batches] loss: 0.409\n",
      "[Epoch 113,   100 Mini Batches] loss: 0.393\n",
      "[Epoch 113,   150 Mini Batches] loss: 0.410\n",
      "tensor(0.4051)\n",
      "[Epoch 114,    50 Mini Batches] loss: 0.400\n",
      "[Epoch 114,   100 Mini Batches] loss: 0.401\n",
      "[Epoch 114,   150 Mini Batches] loss: 0.423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4089)\n",
      "[Epoch 115,    50 Mini Batches] loss: 0.406\n",
      "[Epoch 115,   100 Mini Batches] loss: 0.405\n",
      "[Epoch 115,   150 Mini Batches] loss: 0.411\n",
      "tensor(0.4074)\n",
      "[Epoch 116,    50 Mini Batches] loss: 0.410\n",
      "[Epoch 116,   100 Mini Batches] loss: 0.412\n",
      "[Epoch 116,   150 Mini Batches] loss: 0.405\n",
      "tensor(0.4101)\n",
      "[Epoch 117,    50 Mini Batches] loss: 0.414\n",
      "[Epoch 117,   100 Mini Batches] loss: 0.415\n",
      "[Epoch 117,   150 Mini Batches] loss: 0.396\n",
      "tensor(0.4048)\n",
      "[Epoch 118,    50 Mini Batches] loss: 0.413\n",
      "[Epoch 118,   100 Mini Batches] loss: 0.398\n",
      "[Epoch 118,   150 Mini Batches] loss: 0.395\n",
      "tensor(0.4092)\n",
      "[Epoch 119,    50 Mini Batches] loss: 0.423\n",
      "[Epoch 119,   100 Mini Batches] loss: 0.394\n",
      "[Epoch 119,   150 Mini Batches] loss: 0.392\n",
      "tensor(0.4064)\n",
      "[Epoch 120,    50 Mini Batches] loss: 0.391\n",
      "[Epoch 120,   100 Mini Batches] loss: 0.416\n",
      "[Epoch 120,   150 Mini Batches] loss: 0.408\n",
      "tensor(0.4105)\n",
      "[Epoch 121,    50 Mini Batches] loss: 0.402\n",
      "[Epoch 121,   100 Mini Batches] loss: 0.404\n",
      "[Epoch 121,   150 Mini Batches] loss: 0.399\n",
      "tensor(0.4044)\n",
      "[Epoch 122,    50 Mini Batches] loss: 0.393\n",
      "[Epoch 122,   100 Mini Batches] loss: 0.393\n",
      "[Epoch 122,   150 Mini Batches] loss: 0.421\n",
      "tensor(0.4109)\n",
      "[Epoch 123,    50 Mini Batches] loss: 0.385\n",
      "[Epoch 123,   100 Mini Batches] loss: 0.401\n",
      "[Epoch 123,   150 Mini Batches] loss: 0.402\n",
      "tensor(0.4083)\n",
      "[Epoch 124,    50 Mini Batches] loss: 0.408\n",
      "[Epoch 124,   100 Mini Batches] loss: 0.409\n",
      "[Epoch 124,   150 Mini Batches] loss: 0.403\n",
      "tensor(0.4166)\n",
      "[Epoch 125,    50 Mini Batches] loss: 0.407\n",
      "[Epoch 125,   100 Mini Batches] loss: 0.398\n",
      "[Epoch 125,   150 Mini Batches] loss: 0.409\n",
      "tensor(0.4098)\n",
      "[Epoch 126,    50 Mini Batches] loss: 0.378\n",
      "[Epoch 126,   100 Mini Batches] loss: 0.402\n",
      "[Epoch 126,   150 Mini Batches] loss: 0.419\n",
      "tensor(0.4084)\n",
      "[Epoch 127,    50 Mini Batches] loss: 0.398\n",
      "[Epoch 127,   100 Mini Batches] loss: 0.409\n",
      "[Epoch 127,   150 Mini Batches] loss: 0.403\n",
      "tensor(0.4059)\n",
      "[Epoch 128,    50 Mini Batches] loss: 0.409\n",
      "[Epoch 128,   100 Mini Batches] loss: 0.383\n",
      "[Epoch 128,   150 Mini Batches] loss: 0.417\n",
      "tensor(0.4129)\n",
      "[Epoch 129,    50 Mini Batches] loss: 0.400\n",
      "[Epoch 129,   100 Mini Batches] loss: 0.405\n",
      "[Epoch 129,   150 Mini Batches] loss: 0.410\n",
      "tensor(0.4072)\n",
      "[Epoch 130,    50 Mini Batches] loss: 0.400\n",
      "[Epoch 130,   100 Mini Batches] loss: 0.407\n",
      "[Epoch 130,   150 Mini Batches] loss: 0.412\n",
      "tensor(0.4099)\n",
      "[Epoch 131,    50 Mini Batches] loss: 0.410\n",
      "[Epoch 131,   100 Mini Batches] loss: 0.394\n",
      "[Epoch 131,   150 Mini Batches] loss: 0.390\n",
      "tensor(0.4084)\n",
      "[Epoch 132,    50 Mini Batches] loss: 0.402\n",
      "[Epoch 132,   100 Mini Batches] loss: 0.404\n",
      "[Epoch 132,   150 Mini Batches] loss: 0.405\n",
      "tensor(0.4108)\n",
      "[Epoch 133,    50 Mini Batches] loss: 0.394\n",
      "[Epoch 133,   100 Mini Batches] loss: 0.408\n",
      "[Epoch 133,   150 Mini Batches] loss: 0.417\n",
      "tensor(0.4101)\n",
      "[Epoch 134,    50 Mini Batches] loss: 0.401\n",
      "[Epoch 134,   100 Mini Batches] loss: 0.409\n",
      "[Epoch 134,   150 Mini Batches] loss: 0.392\n",
      "tensor(0.4086)\n",
      "[Epoch 135,    50 Mini Batches] loss: 0.414\n",
      "[Epoch 135,   100 Mini Batches] loss: 0.402\n",
      "[Epoch 135,   150 Mini Batches] loss: 0.385\n",
      "tensor(0.4086)\n",
      "[Epoch 136,    50 Mini Batches] loss: 0.397\n",
      "[Epoch 136,   100 Mini Batches] loss: 0.407\n",
      "[Epoch 136,   150 Mini Batches] loss: 0.413\n",
      "tensor(0.4083)\n",
      "[Epoch 137,    50 Mini Batches] loss: 0.411\n",
      "[Epoch 137,   100 Mini Batches] loss: 0.393\n",
      "[Epoch 137,   150 Mini Batches] loss: 0.413\n",
      "tensor(0.4063)\n",
      "[Epoch 138,    50 Mini Batches] loss: 0.409\n",
      "[Epoch 138,   100 Mini Batches] loss: 0.403\n",
      "[Epoch 138,   150 Mini Batches] loss: 0.404\n",
      "tensor(0.4085)\n",
      "[Epoch 139,    50 Mini Batches] loss: 0.397\n",
      "[Epoch 139,   100 Mini Batches] loss: 0.392\n",
      "[Epoch 139,   150 Mini Batches] loss: 0.413\n",
      "tensor(0.4120)\n",
      "[Epoch 140,    50 Mini Batches] loss: 0.406\n",
      "[Epoch 140,   100 Mini Batches] loss: 0.406\n",
      "[Epoch 140,   150 Mini Batches] loss: 0.409\n",
      "tensor(0.4075)\n",
      "[Epoch 141,    50 Mini Batches] loss: 0.408\n",
      "[Epoch 141,   100 Mini Batches] loss: 0.397\n",
      "[Epoch 141,   150 Mini Batches] loss: 0.401\n",
      "tensor(0.4049)\n",
      "[Epoch 142,    50 Mini Batches] loss: 0.408\n",
      "[Epoch 142,   100 Mini Batches] loss: 0.393\n",
      "[Epoch 142,   150 Mini Batches] loss: 0.405\n",
      "tensor(0.4080)\n",
      "[Epoch 143,    50 Mini Batches] loss: 0.402\n",
      "[Epoch 143,   100 Mini Batches] loss: 0.401\n",
      "[Epoch 143,   150 Mini Batches] loss: 0.409\n",
      "tensor(0.4094)\n",
      "[Epoch 144,    50 Mini Batches] loss: 0.401\n",
      "[Epoch 144,   100 Mini Batches] loss: 0.395\n",
      "[Epoch 144,   150 Mini Batches] loss: 0.405\n",
      "tensor(0.4109)\n",
      "[Epoch 145,    50 Mini Batches] loss: 0.383\n",
      "[Epoch 145,   100 Mini Batches] loss: 0.424\n",
      "[Epoch 145,   150 Mini Batches] loss: 0.407\n",
      "tensor(0.4079)\n",
      "[Epoch 146,    50 Mini Batches] loss: 0.386\n",
      "[Epoch 146,   100 Mini Batches] loss: 0.404\n",
      "[Epoch 146,   150 Mini Batches] loss: 0.405\n",
      "tensor(0.4097)\n",
      "[Epoch 147,    50 Mini Batches] loss: 0.411\n",
      "[Epoch 147,   100 Mini Batches] loss: 0.401\n",
      "[Epoch 147,   150 Mini Batches] loss: 0.395\n",
      "tensor(0.4081)\n",
      "[Epoch 148,    50 Mini Batches] loss: 0.390\n",
      "[Epoch 148,   100 Mini Batches] loss: 0.409\n",
      "[Epoch 148,   150 Mini Batches] loss: 0.407\n",
      "tensor(0.4071)\n",
      "[Epoch 149,    50 Mini Batches] loss: 0.404\n",
      "[Epoch 149,   100 Mini Batches] loss: 0.397\n",
      "[Epoch 149,   150 Mini Batches] loss: 0.395\n",
      "tensor(0.4077)\n",
      "[Epoch 150,    50 Mini Batches] loss: 0.405\n",
      "[Epoch 150,   100 Mini Batches] loss: 0.413\n",
      "[Epoch 150,   150 Mini Batches] loss: 0.399\n",
      "tensor(0.4100)\n",
      "[Epoch 151,    50 Mini Batches] loss: 0.405\n",
      "[Epoch 151,   100 Mini Batches] loss: 0.407\n",
      "[Epoch 151,   150 Mini Batches] loss: 0.399\n",
      "tensor(0.4097)\n",
      "[Epoch 152,    50 Mini Batches] loss: 0.396\n",
      "[Epoch 152,   100 Mini Batches] loss: 0.415\n",
      "[Epoch 152,   150 Mini Batches] loss: 0.387\n",
      "tensor(0.4145)\n",
      "[Epoch 153,    50 Mini Batches] loss: 0.395\n",
      "[Epoch 153,   100 Mini Batches] loss: 0.388\n",
      "[Epoch 153,   150 Mini Batches] loss: 0.416\n",
      "tensor(0.4076)\n",
      "[Epoch 154,    50 Mini Batches] loss: 0.398\n",
      "[Epoch 154,   100 Mini Batches] loss: 0.401\n",
      "[Epoch 154,   150 Mini Batches] loss: 0.409\n",
      "tensor(0.4082)\n",
      "[Epoch 155,    50 Mini Batches] loss: 0.390\n",
      "[Epoch 155,   100 Mini Batches] loss: 0.399\n",
      "[Epoch 155,   150 Mini Batches] loss: 0.407\n",
      "tensor(0.4081)\n",
      "[Epoch 156,    50 Mini Batches] loss: 0.405\n",
      "[Epoch 156,   100 Mini Batches] loss: 0.409\n",
      "[Epoch 156,   150 Mini Batches] loss: 0.389\n",
      "tensor(0.4072)\n",
      "[Epoch 157,    50 Mini Batches] loss: 0.421\n",
      "[Epoch 157,   100 Mini Batches] loss: 0.408\n",
      "[Epoch 157,   150 Mini Batches] loss: 0.392\n",
      "tensor(0.4100)\n",
      "[Epoch 158,    50 Mini Batches] loss: 0.400\n",
      "[Epoch 158,   100 Mini Batches] loss: 0.398\n",
      "[Epoch 158,   150 Mini Batches] loss: 0.391\n",
      "tensor(0.4091)\n",
      "[Epoch 159,    50 Mini Batches] loss: 0.390\n",
      "[Epoch 159,   100 Mini Batches] loss: 0.402\n",
      "[Epoch 159,   150 Mini Batches] loss: 0.410\n",
      "tensor(0.4067)\n",
      "[Epoch 160,    50 Mini Batches] loss: 0.380\n",
      "[Epoch 160,   100 Mini Batches] loss: 0.400\n",
      "[Epoch 160,   150 Mini Batches] loss: 0.431\n",
      "tensor(0.4090)\n",
      "[Epoch 161,    50 Mini Batches] loss: 0.396\n",
      "[Epoch 161,   100 Mini Batches] loss: 0.402\n",
      "[Epoch 161,   150 Mini Batches] loss: 0.397\n",
      "tensor(0.4054)\n",
      "[Epoch 162,    50 Mini Batches] loss: 0.397\n",
      "[Epoch 162,   100 Mini Batches] loss: 0.405\n",
      "[Epoch 162,   150 Mini Batches] loss: 0.410\n",
      "tensor(0.4130)\n",
      "[Epoch 163,    50 Mini Batches] loss: 0.399\n",
      "[Epoch 163,   100 Mini Batches] loss: 0.415\n",
      "[Epoch 163,   150 Mini Batches] loss: 0.405\n",
      "tensor(0.4057)\n",
      "[Epoch 164,    50 Mini Batches] loss: 0.406\n",
      "[Epoch 164,   100 Mini Batches] loss: 0.399\n",
      "[Epoch 164,   150 Mini Batches] loss: 0.401\n",
      "tensor(0.4104)\n",
      "[Epoch 165,    50 Mini Batches] loss: 0.399\n",
      "[Epoch 165,   100 Mini Batches] loss: 0.395\n",
      "[Epoch 165,   150 Mini Batches] loss: 0.415\n",
      "tensor(0.4085)\n",
      "[Epoch 166,    50 Mini Batches] loss: 0.398\n",
      "[Epoch 166,   100 Mini Batches] loss: 0.395\n",
      "[Epoch 166,   150 Mini Batches] loss: 0.408\n",
      "tensor(0.4096)\n",
      "[Epoch 167,    50 Mini Batches] loss: 0.406\n",
      "[Epoch 167,   100 Mini Batches] loss: 0.418\n",
      "[Epoch 167,   150 Mini Batches] loss: 0.392\n",
      "tensor(0.4119)\n",
      "[Epoch 168,    50 Mini Batches] loss: 0.407\n",
      "[Epoch 168,   100 Mini Batches] loss: 0.406\n",
      "[Epoch 168,   150 Mini Batches] loss: 0.392\n",
      "tensor(0.4103)\n",
      "[Epoch 169,    50 Mini Batches] loss: 0.396\n",
      "[Epoch 169,   100 Mini Batches] loss: 0.423\n",
      "[Epoch 169,   150 Mini Batches] loss: 0.397\n",
      "tensor(0.4098)\n",
      "[Epoch 170,    50 Mini Batches] loss: 0.387\n",
      "[Epoch 170,   100 Mini Batches] loss: 0.419\n",
      "[Epoch 170,   150 Mini Batches] loss: 0.408\n",
      "tensor(0.4120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 171,    50 Mini Batches] loss: 0.400\n",
      "[Epoch 171,   100 Mini Batches] loss: 0.405\n",
      "[Epoch 171,   150 Mini Batches] loss: 0.399\n",
      "tensor(0.4111)\n",
      "[Epoch 172,    50 Mini Batches] loss: 0.412\n",
      "[Epoch 172,   100 Mini Batches] loss: 0.389\n",
      "[Epoch 172,   150 Mini Batches] loss: 0.410\n",
      "tensor(0.4080)\n",
      "[Epoch 173,    50 Mini Batches] loss: 0.403\n",
      "[Epoch 173,   100 Mini Batches] loss: 0.394\n",
      "[Epoch 173,   150 Mini Batches] loss: 0.405\n",
      "tensor(0.4087)\n",
      "[Epoch 174,    50 Mini Batches] loss: 0.418\n",
      "[Epoch 174,   100 Mini Batches] loss: 0.396\n",
      "[Epoch 174,   150 Mini Batches] loss: 0.398\n",
      "tensor(0.4130)\n",
      "[Epoch 175,    50 Mini Batches] loss: 0.387\n",
      "[Epoch 175,   100 Mini Batches] loss: 0.413\n",
      "[Epoch 175,   150 Mini Batches] loss: 0.399\n",
      "tensor(0.4121)\n",
      "[Epoch 176,    50 Mini Batches] loss: 0.381\n",
      "[Epoch 176,   100 Mini Batches] loss: 0.418\n",
      "[Epoch 176,   150 Mini Batches] loss: 0.390\n",
      "tensor(0.4106)\n",
      "[Epoch 177,    50 Mini Batches] loss: 0.403\n",
      "[Epoch 177,   100 Mini Batches] loss: 0.406\n",
      "[Epoch 177,   150 Mini Batches] loss: 0.397\n",
      "tensor(0.4082)\n",
      "[Epoch 178,    50 Mini Batches] loss: 0.382\n",
      "[Epoch 178,   100 Mini Batches] loss: 0.405\n",
      "[Epoch 178,   150 Mini Batches] loss: 0.400\n",
      "tensor(0.4088)\n",
      "[Epoch 179,    50 Mini Batches] loss: 0.397\n",
      "[Epoch 179,   100 Mini Batches] loss: 0.403\n",
      "[Epoch 179,   150 Mini Batches] loss: 0.415\n",
      "tensor(0.4104)\n",
      "[Epoch 180,    50 Mini Batches] loss: 0.415\n",
      "[Epoch 180,   100 Mini Batches] loss: 0.396\n",
      "[Epoch 180,   150 Mini Batches] loss: 0.388\n",
      "tensor(0.4073)\n",
      "[Epoch 181,    50 Mini Batches] loss: 0.384\n",
      "[Epoch 181,   100 Mini Batches] loss: 0.405\n",
      "[Epoch 181,   150 Mini Batches] loss: 0.399\n",
      "tensor(0.4108)\n",
      "[Epoch 182,    50 Mini Batches] loss: 0.374\n",
      "[Epoch 182,   100 Mini Batches] loss: 0.400\n",
      "[Epoch 182,   150 Mini Batches] loss: 0.422\n",
      "tensor(0.4105)\n",
      "[Epoch 183,    50 Mini Batches] loss: 0.393\n",
      "[Epoch 183,   100 Mini Batches] loss: 0.416\n",
      "[Epoch 183,   150 Mini Batches] loss: 0.409\n",
      "tensor(0.4086)\n",
      "[Epoch 184,    50 Mini Batches] loss: 0.394\n",
      "[Epoch 184,   100 Mini Batches] loss: 0.418\n",
      "[Epoch 184,   150 Mini Batches] loss: 0.399\n",
      "tensor(0.4099)\n",
      "[Epoch 185,    50 Mini Batches] loss: 0.386\n",
      "[Epoch 185,   100 Mini Batches] loss: 0.399\n",
      "[Epoch 185,   150 Mini Batches] loss: 0.413\n",
      "tensor(0.4122)\n",
      "[Epoch 186,    50 Mini Batches] loss: 0.400\n",
      "[Epoch 186,   100 Mini Batches] loss: 0.397\n",
      "[Epoch 186,   150 Mini Batches] loss: 0.413\n",
      "tensor(0.4134)\n",
      "[Epoch 187,    50 Mini Batches] loss: 0.386\n",
      "[Epoch 187,   100 Mini Batches] loss: 0.402\n",
      "[Epoch 187,   150 Mini Batches] loss: 0.413\n",
      "tensor(0.4071)\n",
      "[Epoch 188,    50 Mini Batches] loss: 0.396\n",
      "[Epoch 188,   100 Mini Batches] loss: 0.399\n",
      "[Epoch 188,   150 Mini Batches] loss: 0.401\n",
      "tensor(0.4120)\n",
      "[Epoch 189,    50 Mini Batches] loss: 0.415\n",
      "[Epoch 189,   100 Mini Batches] loss: 0.405\n",
      "[Epoch 189,   150 Mini Batches] loss: 0.387\n",
      "tensor(0.4097)\n",
      "[Epoch 190,    50 Mini Batches] loss: 0.408\n",
      "[Epoch 190,   100 Mini Batches] loss: 0.393\n",
      "[Epoch 190,   150 Mini Batches] loss: 0.397\n",
      "tensor(0.4097)\n",
      "[Epoch 191,    50 Mini Batches] loss: 0.392\n",
      "[Epoch 191,   100 Mini Batches] loss: 0.394\n",
      "[Epoch 191,   150 Mini Batches] loss: 0.401\n",
      "tensor(0.4132)\n",
      "[Epoch 192,    50 Mini Batches] loss: 0.393\n",
      "[Epoch 192,   100 Mini Batches] loss: 0.399\n",
      "[Epoch 192,   150 Mini Batches] loss: 0.404\n",
      "tensor(0.4082)\n",
      "[Epoch 193,    50 Mini Batches] loss: 0.407\n",
      "[Epoch 193,   100 Mini Batches] loss: 0.391\n",
      "[Epoch 193,   150 Mini Batches] loss: 0.392\n",
      "tensor(0.4093)\n",
      "[Epoch 194,    50 Mini Batches] loss: 0.396\n",
      "[Epoch 194,   100 Mini Batches] loss: 0.391\n",
      "[Epoch 194,   150 Mini Batches] loss: 0.402\n",
      "tensor(0.4118)\n",
      "[Epoch 195,    50 Mini Batches] loss: 0.390\n",
      "[Epoch 195,   100 Mini Batches] loss: 0.399\n",
      "[Epoch 195,   150 Mini Batches] loss: 0.418\n",
      "tensor(0.4074)\n",
      "[Epoch 196,    50 Mini Batches] loss: 0.408\n",
      "[Epoch 196,   100 Mini Batches] loss: 0.403\n",
      "[Epoch 196,   150 Mini Batches] loss: 0.395\n",
      "tensor(0.4086)\n",
      "[Epoch 197,    50 Mini Batches] loss: 0.392\n",
      "[Epoch 197,   100 Mini Batches] loss: 0.406\n",
      "[Epoch 197,   150 Mini Batches] loss: 0.391\n",
      "tensor(0.4113)\n",
      "[Epoch 198,    50 Mini Batches] loss: 0.382\n",
      "[Epoch 198,   100 Mini Batches] loss: 0.396\n",
      "[Epoch 198,   150 Mini Batches] loss: 0.399\n",
      "tensor(0.4171)\n",
      "[Epoch 199,    50 Mini Batches] loss: 0.393\n",
      "[Epoch 199,   100 Mini Batches] loss: 0.400\n",
      "[Epoch 199,   150 Mini Batches] loss: 0.414\n",
      "tensor(0.4091)\n",
      "[Epoch 200,    50 Mini Batches] loss: 0.394\n",
      "[Epoch 200,   100 Mini Batches] loss: 0.400\n",
      "[Epoch 200,   150 Mini Batches] loss: 0.404\n",
      "tensor(0.4111)\n",
      "[Epoch 201,    50 Mini Batches] loss: 0.402\n",
      "[Epoch 201,   100 Mini Batches] loss: 0.407\n",
      "[Epoch 201,   150 Mini Batches] loss: 0.384\n",
      "tensor(0.4090)\n",
      "[Epoch 202,    50 Mini Batches] loss: 0.409\n",
      "[Epoch 202,   100 Mini Batches] loss: 0.392\n",
      "[Epoch 202,   150 Mini Batches] loss: 0.414\n",
      "tensor(0.4109)\n",
      "[Epoch 203,    50 Mini Batches] loss: 0.405\n",
      "[Epoch 203,   100 Mini Batches] loss: 0.399\n",
      "[Epoch 203,   150 Mini Batches] loss: 0.401\n",
      "tensor(0.4131)\n",
      "[Epoch 204,    50 Mini Batches] loss: 0.401\n",
      "[Epoch 204,   100 Mini Batches] loss: 0.394\n",
      "[Epoch 204,   150 Mini Batches] loss: 0.404\n",
      "tensor(0.4083)\n",
      "[Epoch 205,    50 Mini Batches] loss: 0.401\n",
      "[Epoch 205,   100 Mini Batches] loss: 0.408\n",
      "[Epoch 205,   150 Mini Batches] loss: 0.400\n",
      "tensor(0.4089)\n",
      "[Epoch 206,    50 Mini Batches] loss: 0.389\n",
      "[Epoch 206,   100 Mini Batches] loss: 0.404\n",
      "[Epoch 206,   150 Mini Batches] loss: 0.403\n",
      "tensor(0.4087)\n",
      "[Epoch 207,    50 Mini Batches] loss: 0.407\n",
      "[Epoch 207,   100 Mini Batches] loss: 0.405\n",
      "[Epoch 207,   150 Mini Batches] loss: 0.392\n",
      "tensor(0.4116)\n",
      "[Epoch 208,    50 Mini Batches] loss: 0.393\n",
      "[Epoch 208,   100 Mini Batches] loss: 0.399\n",
      "[Epoch 208,   150 Mini Batches] loss: 0.391\n",
      "tensor(0.4141)\n",
      "[Epoch 209,    50 Mini Batches] loss: 0.370\n",
      "[Epoch 209,   100 Mini Batches] loss: 0.409\n",
      "[Epoch 209,   150 Mini Batches] loss: 0.402\n",
      "tensor(0.4110)\n",
      "[Epoch 210,    50 Mini Batches] loss: 0.404\n",
      "[Epoch 210,   100 Mini Batches] loss: 0.398\n",
      "[Epoch 210,   150 Mini Batches] loss: 0.402\n",
      "tensor(0.4137)\n",
      "[Epoch 211,    50 Mini Batches] loss: 0.396\n",
      "[Epoch 211,   100 Mini Batches] loss: 0.394\n",
      "[Epoch 211,   150 Mini Batches] loss: 0.407\n",
      "tensor(0.4088)\n",
      "[Epoch 212,    50 Mini Batches] loss: 0.393\n",
      "[Epoch 212,   100 Mini Batches] loss: 0.389\n",
      "[Epoch 212,   150 Mini Batches] loss: 0.415\n",
      "tensor(0.4107)\n",
      "[Epoch 213,    50 Mini Batches] loss: 0.401\n",
      "[Epoch 213,   100 Mini Batches] loss: 0.386\n",
      "[Epoch 213,   150 Mini Batches] loss: 0.383\n",
      "tensor(0.4145)\n",
      "[Epoch 214,    50 Mini Batches] loss: 0.393\n",
      "[Epoch 214,   100 Mini Batches] loss: 0.414\n",
      "[Epoch 214,   150 Mini Batches] loss: 0.398\n",
      "tensor(0.4092)\n",
      "[Epoch 215,    50 Mini Batches] loss: 0.392\n",
      "[Epoch 215,   100 Mini Batches] loss: 0.405\n",
      "[Epoch 215,   150 Mini Batches] loss: 0.402\n",
      "tensor(0.4088)\n",
      "[Epoch 216,    50 Mini Batches] loss: 0.406\n",
      "[Epoch 216,   100 Mini Batches] loss: 0.407\n",
      "[Epoch 216,   150 Mini Batches] loss: 0.386\n",
      "tensor(0.4135)\n",
      "[Epoch 217,    50 Mini Batches] loss: 0.396\n",
      "[Epoch 217,   100 Mini Batches] loss: 0.399\n",
      "[Epoch 217,   150 Mini Batches] loss: 0.394\n",
      "tensor(0.4113)\n",
      "[Epoch 218,    50 Mini Batches] loss: 0.393\n",
      "[Epoch 218,   100 Mini Batches] loss: 0.398\n",
      "[Epoch 218,   150 Mini Batches] loss: 0.394\n",
      "tensor(0.4124)\n",
      "[Epoch 219,    50 Mini Batches] loss: 0.391\n",
      "[Epoch 219,   100 Mini Batches] loss: 0.402\n",
      "[Epoch 219,   150 Mini Batches] loss: 0.408\n",
      "tensor(0.4096)\n",
      "[Epoch 220,    50 Mini Batches] loss: 0.382\n",
      "[Epoch 220,   100 Mini Batches] loss: 0.391\n",
      "[Epoch 220,   150 Mini Batches] loss: 0.413\n",
      "tensor(0.4095)\n",
      "[Epoch 221,    50 Mini Batches] loss: 0.387\n",
      "[Epoch 221,   100 Mini Batches] loss: 0.400\n",
      "[Epoch 221,   150 Mini Batches] loss: 0.405\n",
      "tensor(0.4091)\n",
      "[Epoch 222,    50 Mini Batches] loss: 0.411\n",
      "[Epoch 222,   100 Mini Batches] loss: 0.392\n",
      "[Epoch 222,   150 Mini Batches] loss: 0.380\n",
      "tensor(0.4105)\n",
      "[Epoch 223,    50 Mini Batches] loss: 0.391\n",
      "[Epoch 223,   100 Mini Batches] loss: 0.391\n",
      "[Epoch 223,   150 Mini Batches] loss: 0.392\n",
      "tensor(0.4130)\n",
      "[Epoch 224,    50 Mini Batches] loss: 0.396\n",
      "[Epoch 224,   100 Mini Batches] loss: 0.394\n",
      "[Epoch 224,   150 Mini Batches] loss: 0.395\n",
      "tensor(0.4126)\n",
      "[Epoch 225,    50 Mini Batches] loss: 0.391\n",
      "[Epoch 225,   100 Mini Batches] loss: 0.402\n",
      "[Epoch 225,   150 Mini Batches] loss: 0.395\n",
      "tensor(0.4089)\n",
      "[Epoch 226,    50 Mini Batches] loss: 0.404\n",
      "[Epoch 226,   100 Mini Batches] loss: 0.403\n",
      "[Epoch 226,   150 Mini Batches] loss: 0.383\n",
      "tensor(0.4101)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 227,    50 Mini Batches] loss: 0.408\n",
      "[Epoch 227,   100 Mini Batches] loss: 0.402\n",
      "[Epoch 227,   150 Mini Batches] loss: 0.392\n",
      "tensor(0.4082)\n",
      "[Epoch 228,    50 Mini Batches] loss: 0.387\n",
      "[Epoch 228,   100 Mini Batches] loss: 0.400\n",
      "[Epoch 228,   150 Mini Batches] loss: 0.397\n",
      "tensor(0.4104)\n",
      "[Epoch 229,    50 Mini Batches] loss: 0.393\n",
      "[Epoch 229,   100 Mini Batches] loss: 0.393\n",
      "[Epoch 229,   150 Mini Batches] loss: 0.407\n",
      "tensor(0.4121)\n",
      "[Epoch 230,    50 Mini Batches] loss: 0.392\n",
      "[Epoch 230,   100 Mini Batches] loss: 0.395\n",
      "[Epoch 230,   150 Mini Batches] loss: 0.391\n",
      "tensor(0.4084)\n",
      "[Epoch 231,    50 Mini Batches] loss: 0.379\n",
      "[Epoch 231,   100 Mini Batches] loss: 0.395\n",
      "[Epoch 231,   150 Mini Batches] loss: 0.405\n",
      "tensor(0.4162)\n",
      "[Epoch 232,    50 Mini Batches] loss: 0.395\n",
      "[Epoch 232,   100 Mini Batches] loss: 0.386\n",
      "[Epoch 232,   150 Mini Batches] loss: 0.400\n",
      "tensor(0.4139)\n",
      "[Epoch 233,    50 Mini Batches] loss: 0.402\n",
      "[Epoch 233,   100 Mini Batches] loss: 0.400\n",
      "[Epoch 233,   150 Mini Batches] loss: 0.403\n",
      "tensor(0.4089)\n",
      "[Epoch 234,    50 Mini Batches] loss: 0.406\n",
      "[Epoch 234,   100 Mini Batches] loss: 0.391\n",
      "[Epoch 234,   150 Mini Batches] loss: 0.388\n",
      "tensor(0.4138)\n",
      "[Epoch 235,    50 Mini Batches] loss: 0.392\n",
      "[Epoch 235,   100 Mini Batches] loss: 0.401\n",
      "[Epoch 235,   150 Mini Batches] loss: 0.395\n",
      "tensor(0.4108)\n",
      "[Epoch 236,    50 Mini Batches] loss: 0.401\n",
      "[Epoch 236,   100 Mini Batches] loss: 0.402\n",
      "[Epoch 236,   150 Mini Batches] loss: 0.377\n",
      "tensor(0.4084)\n",
      "[Epoch 237,    50 Mini Batches] loss: 0.405\n",
      "[Epoch 237,   100 Mini Batches] loss: 0.386\n",
      "[Epoch 237,   150 Mini Batches] loss: 0.396\n",
      "tensor(0.4089)\n",
      "[Epoch 238,    50 Mini Batches] loss: 0.392\n",
      "[Epoch 238,   100 Mini Batches] loss: 0.394\n",
      "[Epoch 238,   150 Mini Batches] loss: 0.395\n",
      "tensor(0.4127)\n",
      "[Epoch 239,    50 Mini Batches] loss: 0.386\n",
      "[Epoch 239,   100 Mini Batches] loss: 0.395\n",
      "[Epoch 239,   150 Mini Batches] loss: 0.405\n",
      "tensor(0.4101)\n",
      "[Epoch 240,    50 Mini Batches] loss: 0.384\n",
      "[Epoch 240,   100 Mini Batches] loss: 0.398\n",
      "[Epoch 240,   150 Mini Batches] loss: 0.411\n",
      "tensor(0.4119)\n",
      "[Epoch 241,    50 Mini Batches] loss: 0.395\n",
      "[Epoch 241,   100 Mini Batches] loss: 0.380\n",
      "[Epoch 241,   150 Mini Batches] loss: 0.389\n",
      "tensor(0.4107)\n",
      "[Epoch 242,    50 Mini Batches] loss: 0.394\n",
      "[Epoch 242,   100 Mini Batches] loss: 0.391\n",
      "[Epoch 242,   150 Mini Batches] loss: 0.399\n",
      "tensor(0.4115)\n",
      "[Epoch 243,    50 Mini Batches] loss: 0.397\n",
      "[Epoch 243,   100 Mini Batches] loss: 0.386\n",
      "[Epoch 243,   150 Mini Batches] loss: 0.416\n",
      "tensor(0.4073)\n",
      "[Epoch 244,    50 Mini Batches] loss: 0.392\n",
      "[Epoch 244,   100 Mini Batches] loss: 0.405\n",
      "[Epoch 244,   150 Mini Batches] loss: 0.392\n",
      "tensor(0.4109)\n",
      "[Epoch 245,    50 Mini Batches] loss: 0.396\n",
      "[Epoch 245,   100 Mini Batches] loss: 0.395\n",
      "[Epoch 245,   150 Mini Batches] loss: 0.383\n",
      "tensor(0.4108)\n",
      "[Epoch 246,    50 Mini Batches] loss: 0.404\n",
      "[Epoch 246,   100 Mini Batches] loss: 0.417\n",
      "[Epoch 246,   150 Mini Batches] loss: 0.382\n",
      "tensor(0.4101)\n",
      "[Epoch 247,    50 Mini Batches] loss: 0.388\n",
      "[Epoch 247,   100 Mini Batches] loss: 0.403\n",
      "[Epoch 247,   150 Mini Batches] loss: 0.382\n",
      "tensor(0.4136)\n",
      "[Epoch 248,    50 Mini Batches] loss: 0.401\n",
      "[Epoch 248,   100 Mini Batches] loss: 0.388\n",
      "[Epoch 248,   150 Mini Batches] loss: 0.396\n",
      "tensor(0.4151)\n",
      "[Epoch 249,    50 Mini Batches] loss: 0.398\n",
      "[Epoch 249,   100 Mini Batches] loss: 0.383\n",
      "[Epoch 249,   150 Mini Batches] loss: 0.399\n",
      "tensor(0.4114)\n",
      "[Epoch 250,    50 Mini Batches] loss: 0.400\n",
      "[Epoch 250,   100 Mini Batches] loss: 0.403\n",
      "[Epoch 250,   150 Mini Batches] loss: 0.382\n",
      "tensor(0.4101)\n",
      "[Epoch 251,    50 Mini Batches] loss: 0.393\n",
      "[Epoch 251,   100 Mini Batches] loss: 0.411\n",
      "[Epoch 251,   150 Mini Batches] loss: 0.395\n",
      "tensor(0.4124)\n",
      "[Epoch 252,    50 Mini Batches] loss: 0.391\n",
      "[Epoch 252,   100 Mini Batches] loss: 0.404\n",
      "[Epoch 252,   150 Mini Batches] loss: 0.391\n",
      "tensor(0.4149)\n",
      "[Epoch 253,    50 Mini Batches] loss: 0.402\n",
      "[Epoch 253,   100 Mini Batches] loss: 0.398\n",
      "[Epoch 253,   150 Mini Batches] loss: 0.380\n",
      "tensor(0.4134)\n",
      "[Epoch 254,    50 Mini Batches] loss: 0.397\n",
      "[Epoch 254,   100 Mini Batches] loss: 0.394\n",
      "[Epoch 254,   150 Mini Batches] loss: 0.404\n",
      "tensor(0.4130)\n",
      "[Epoch 255,    50 Mini Batches] loss: 0.404\n",
      "[Epoch 255,   100 Mini Batches] loss: 0.392\n",
      "[Epoch 255,   150 Mini Batches] loss: 0.399\n",
      "tensor(0.4099)\n",
      "[Epoch 256,    50 Mini Batches] loss: 0.384\n",
      "[Epoch 256,   100 Mini Batches] loss: 0.410\n",
      "[Epoch 256,   150 Mini Batches] loss: 0.401\n",
      "tensor(0.4147)\n",
      "[Epoch 257,    50 Mini Batches] loss: 0.397\n",
      "[Epoch 257,   100 Mini Batches] loss: 0.397\n",
      "[Epoch 257,   150 Mini Batches] loss: 0.387\n",
      "tensor(0.4170)\n",
      "[Epoch 258,    50 Mini Batches] loss: 0.406\n",
      "[Epoch 258,   100 Mini Batches] loss: 0.403\n",
      "[Epoch 258,   150 Mini Batches] loss: 0.381\n",
      "tensor(0.4177)\n",
      "[Epoch 259,    50 Mini Batches] loss: 0.410\n",
      "[Epoch 259,   100 Mini Batches] loss: 0.394\n",
      "[Epoch 259,   150 Mini Batches] loss: 0.376\n",
      "tensor(0.4126)\n",
      "[Epoch 260,    50 Mini Batches] loss: 0.394\n",
      "[Epoch 260,   100 Mini Batches] loss: 0.396\n",
      "[Epoch 260,   150 Mini Batches] loss: 0.400\n",
      "tensor(0.4119)\n",
      "[Epoch 261,    50 Mini Batches] loss: 0.384\n",
      "[Epoch 261,   100 Mini Batches] loss: 0.388\n",
      "[Epoch 261,   150 Mini Batches] loss: 0.408\n",
      "tensor(0.4129)\n",
      "[Epoch 262,    50 Mini Batches] loss: 0.383\n",
      "[Epoch 262,   100 Mini Batches] loss: 0.388\n",
      "[Epoch 262,   150 Mini Batches] loss: 0.404\n",
      "tensor(0.4130)\n",
      "[Epoch 263,    50 Mini Batches] loss: 0.393\n",
      "[Epoch 263,   100 Mini Batches] loss: 0.402\n",
      "[Epoch 263,   150 Mini Batches] loss: 0.391\n",
      "tensor(0.4161)\n",
      "[Epoch 264,    50 Mini Batches] loss: 0.387\n",
      "[Epoch 264,   100 Mini Batches] loss: 0.392\n",
      "[Epoch 264,   150 Mini Batches] loss: 0.407\n",
      "tensor(0.4114)\n",
      "[Epoch 265,    50 Mini Batches] loss: 0.385\n",
      "[Epoch 265,   100 Mini Batches] loss: 0.403\n",
      "[Epoch 265,   150 Mini Batches] loss: 0.384\n",
      "tensor(0.4160)\n",
      "[Epoch 266,    50 Mini Batches] loss: 0.384\n",
      "[Epoch 266,   100 Mini Batches] loss: 0.399\n",
      "[Epoch 266,   150 Mini Batches] loss: 0.393\n",
      "tensor(0.4126)\n",
      "[Epoch 267,    50 Mini Batches] loss: 0.394\n",
      "[Epoch 267,   100 Mini Batches] loss: 0.391\n",
      "[Epoch 267,   150 Mini Batches] loss: 0.400\n",
      "tensor(0.4121)\n",
      "[Epoch 268,    50 Mini Batches] loss: 0.389\n",
      "[Epoch 268,   100 Mini Batches] loss: 0.402\n",
      "[Epoch 268,   150 Mini Batches] loss: 0.402\n",
      "tensor(0.4105)\n",
      "[Epoch 269,    50 Mini Batches] loss: 0.392\n",
      "[Epoch 269,   100 Mini Batches] loss: 0.395\n",
      "[Epoch 269,   150 Mini Batches] loss: 0.385\n",
      "tensor(0.4132)\n",
      "[Epoch 270,    50 Mini Batches] loss: 0.391\n",
      "[Epoch 270,   100 Mini Batches] loss: 0.394\n",
      "[Epoch 270,   150 Mini Batches] loss: 0.392\n",
      "tensor(0.4147)\n",
      "[Epoch 271,    50 Mini Batches] loss: 0.394\n",
      "[Epoch 271,   100 Mini Batches] loss: 0.396\n",
      "[Epoch 271,   150 Mini Batches] loss: 0.414\n",
      "tensor(0.4101)\n",
      "[Epoch 272,    50 Mini Batches] loss: 0.389\n",
      "[Epoch 272,   100 Mini Batches] loss: 0.382\n",
      "[Epoch 272,   150 Mini Batches] loss: 0.418\n",
      "tensor(0.4102)\n",
      "[Epoch 273,    50 Mini Batches] loss: 0.389\n",
      "[Epoch 273,   100 Mini Batches] loss: 0.398\n",
      "[Epoch 273,   150 Mini Batches] loss: 0.387\n",
      "tensor(0.4109)\n",
      "[Epoch 274,    50 Mini Batches] loss: 0.401\n",
      "[Epoch 274,   100 Mini Batches] loss: 0.397\n",
      "[Epoch 274,   150 Mini Batches] loss: 0.383\n",
      "tensor(0.4105)\n",
      "[Epoch 275,    50 Mini Batches] loss: 0.379\n",
      "[Epoch 275,   100 Mini Batches] loss: 0.396\n",
      "[Epoch 275,   150 Mini Batches] loss: 0.408\n",
      "tensor(0.4158)\n",
      "[Epoch 276,    50 Mini Batches] loss: 0.395\n",
      "[Epoch 276,   100 Mini Batches] loss: 0.388\n",
      "[Epoch 276,   150 Mini Batches] loss: 0.390\n",
      "tensor(0.4094)\n",
      "[Epoch 277,    50 Mini Batches] loss: 0.384\n",
      "[Epoch 277,   100 Mini Batches] loss: 0.401\n",
      "[Epoch 277,   150 Mini Batches] loss: 0.399\n",
      "tensor(0.4136)\n",
      "[Epoch 278,    50 Mini Batches] loss: 0.392\n",
      "[Epoch 278,   100 Mini Batches] loss: 0.391\n",
      "[Epoch 278,   150 Mini Batches] loss: 0.402\n",
      "tensor(0.4126)\n",
      "[Epoch 279,    50 Mini Batches] loss: 0.393\n",
      "[Epoch 279,   100 Mini Batches] loss: 0.375\n",
      "[Epoch 279,   150 Mini Batches] loss: 0.395\n",
      "tensor(0.4123)\n",
      "[Epoch 280,    50 Mini Batches] loss: 0.402\n",
      "[Epoch 280,   100 Mini Batches] loss: 0.400\n",
      "[Epoch 280,   150 Mini Batches] loss: 0.385\n",
      "tensor(0.4127)\n",
      "[Epoch 281,    50 Mini Batches] loss: 0.393\n",
      "[Epoch 281,   100 Mini Batches] loss: 0.391\n",
      "[Epoch 281,   150 Mini Batches] loss: 0.395\n",
      "tensor(0.4134)\n",
      "[Epoch 282,    50 Mini Batches] loss: 0.405\n",
      "[Epoch 282,   100 Mini Batches] loss: 0.397\n",
      "[Epoch 282,   150 Mini Batches] loss: 0.393\n",
      "tensor(0.4116)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 283,    50 Mini Batches] loss: 0.389\n",
      "[Epoch 283,   100 Mini Batches] loss: 0.379\n",
      "[Epoch 283,   150 Mini Batches] loss: 0.408\n",
      "tensor(0.4126)\n",
      "[Epoch 284,    50 Mini Batches] loss: 0.398\n",
      "[Epoch 284,   100 Mini Batches] loss: 0.391\n",
      "[Epoch 284,   150 Mini Batches] loss: 0.394\n",
      "tensor(0.4140)\n",
      "[Epoch 285,    50 Mini Batches] loss: 0.388\n",
      "[Epoch 285,   100 Mini Batches] loss: 0.406\n",
      "[Epoch 285,   150 Mini Batches] loss: 0.376\n",
      "tensor(0.4163)\n",
      "[Epoch 286,    50 Mini Batches] loss: 0.375\n",
      "[Epoch 286,   100 Mini Batches] loss: 0.390\n",
      "[Epoch 286,   150 Mini Batches] loss: 0.397\n",
      "tensor(0.4161)\n",
      "[Epoch 287,    50 Mini Batches] loss: 0.383\n",
      "[Epoch 287,   100 Mini Batches] loss: 0.404\n",
      "[Epoch 287,   150 Mini Batches] loss: 0.408\n",
      "tensor(0.4115)\n",
      "[Epoch 288,    50 Mini Batches] loss: 0.388\n",
      "[Epoch 288,   100 Mini Batches] loss: 0.383\n",
      "[Epoch 288,   150 Mini Batches] loss: 0.406\n",
      "tensor(0.4163)\n",
      "[Epoch 289,    50 Mini Batches] loss: 0.392\n",
      "[Epoch 289,   100 Mini Batches] loss: 0.394\n",
      "[Epoch 289,   150 Mini Batches] loss: 0.397\n",
      "tensor(0.4126)\n",
      "[Epoch 290,    50 Mini Batches] loss: 0.389\n",
      "[Epoch 290,   100 Mini Batches] loss: 0.407\n",
      "[Epoch 290,   150 Mini Batches] loss: 0.391\n",
      "tensor(0.4138)\n",
      "[Epoch 291,    50 Mini Batches] loss: 0.394\n",
      "[Epoch 291,   100 Mini Batches] loss: 0.402\n",
      "[Epoch 291,   150 Mini Batches] loss: 0.382\n",
      "tensor(0.4099)\n",
      "[Epoch 292,    50 Mini Batches] loss: 0.370\n",
      "[Epoch 292,   100 Mini Batches] loss: 0.395\n",
      "[Epoch 292,   150 Mini Batches] loss: 0.407\n",
      "tensor(0.4171)\n",
      "[Epoch 293,    50 Mini Batches] loss: 0.386\n",
      "[Epoch 293,   100 Mini Batches] loss: 0.398\n",
      "[Epoch 293,   150 Mini Batches] loss: 0.397\n",
      "tensor(0.4121)\n",
      "[Epoch 294,    50 Mini Batches] loss: 0.388\n",
      "[Epoch 294,   100 Mini Batches] loss: 0.389\n",
      "[Epoch 294,   150 Mini Batches] loss: 0.397\n",
      "tensor(0.4134)\n",
      "[Epoch 295,    50 Mini Batches] loss: 0.388\n",
      "[Epoch 295,   100 Mini Batches] loss: 0.389\n",
      "[Epoch 295,   150 Mini Batches] loss: 0.407\n",
      "tensor(0.4143)\n",
      "[Epoch 296,    50 Mini Batches] loss: 0.402\n",
      "[Epoch 296,   100 Mini Batches] loss: 0.385\n",
      "[Epoch 296,   150 Mini Batches] loss: 0.408\n",
      "tensor(0.4122)\n",
      "[Epoch 297,    50 Mini Batches] loss: 0.378\n",
      "[Epoch 297,   100 Mini Batches] loss: 0.389\n",
      "[Epoch 297,   150 Mini Batches] loss: 0.400\n",
      "tensor(0.4121)\n",
      "[Epoch 298,    50 Mini Batches] loss: 0.391\n",
      "[Epoch 298,   100 Mini Batches] loss: 0.390\n",
      "[Epoch 298,   150 Mini Batches] loss: 0.398\n",
      "tensor(0.4125)\n",
      "[Epoch 299,    50 Mini Batches] loss: 0.393\n",
      "[Epoch 299,   100 Mini Batches] loss: 0.394\n",
      "[Epoch 299,   150 Mini Batches] loss: 0.388\n",
      "tensor(0.4130)\n",
      "[Epoch 300,    50 Mini Batches] loss: 0.402\n",
      "[Epoch 300,   100 Mini Batches] loss: 0.381\n",
      "[Epoch 300,   150 Mini Batches] loss: 0.393\n",
      "tensor(0.4142)\n",
      "[Epoch 301,    50 Mini Batches] loss: 0.383\n",
      "[Epoch 301,   100 Mini Batches] loss: 0.400\n",
      "[Epoch 301,   150 Mini Batches] loss: 0.390\n",
      "tensor(0.4150)\n",
      "[Epoch 302,    50 Mini Batches] loss: 0.396\n",
      "[Epoch 302,   100 Mini Batches] loss: 0.397\n",
      "[Epoch 302,   150 Mini Batches] loss: 0.386\n",
      "tensor(0.4131)\n",
      "[Epoch 303,    50 Mini Batches] loss: 0.398\n",
      "[Epoch 303,   100 Mini Batches] loss: 0.397\n",
      "[Epoch 303,   150 Mini Batches] loss: 0.401\n",
      "tensor(0.4132)\n",
      "[Epoch 304,    50 Mini Batches] loss: 0.394\n",
      "[Epoch 304,   100 Mini Batches] loss: 0.392\n",
      "[Epoch 304,   150 Mini Batches] loss: 0.400\n",
      "tensor(0.4132)\n",
      "[Epoch 305,    50 Mini Batches] loss: 0.385\n",
      "[Epoch 305,   100 Mini Batches] loss: 0.400\n",
      "[Epoch 305,   150 Mini Batches] loss: 0.386\n",
      "tensor(0.4147)\n",
      "[Epoch 306,    50 Mini Batches] loss: 0.387\n",
      "[Epoch 306,   100 Mini Batches] loss: 0.404\n",
      "[Epoch 306,   150 Mini Batches] loss: 0.393\n",
      "tensor(0.4104)\n",
      "[Epoch 307,    50 Mini Batches] loss: 0.391\n",
      "[Epoch 307,   100 Mini Batches] loss: 0.396\n",
      "[Epoch 307,   150 Mini Batches] loss: 0.388\n",
      "tensor(0.4122)\n",
      "[Epoch 308,    50 Mini Batches] loss: 0.403\n",
      "[Epoch 308,   100 Mini Batches] loss: 0.404\n",
      "[Epoch 308,   150 Mini Batches] loss: 0.386\n",
      "tensor(0.4133)\n",
      "[Epoch 309,    50 Mini Batches] loss: 0.390\n",
      "[Epoch 309,   100 Mini Batches] loss: 0.386\n",
      "[Epoch 309,   150 Mini Batches] loss: 0.398\n",
      "tensor(0.4140)\n",
      "[Epoch 310,    50 Mini Batches] loss: 0.396\n",
      "[Epoch 310,   100 Mini Batches] loss: 0.393\n",
      "[Epoch 310,   150 Mini Batches] loss: 0.386\n",
      "tensor(0.4137)\n",
      "[Epoch 311,    50 Mini Batches] loss: 0.371\n",
      "[Epoch 311,   100 Mini Batches] loss: 0.408\n",
      "[Epoch 311,   150 Mini Batches] loss: 0.402\n",
      "tensor(0.4153)\n",
      "[Epoch 312,    50 Mini Batches] loss: 0.390\n",
      "[Epoch 312,   100 Mini Batches] loss: 0.381\n",
      "[Epoch 312,   150 Mini Batches] loss: 0.402\n",
      "tensor(0.4153)\n",
      "[Epoch 313,    50 Mini Batches] loss: 0.396\n",
      "[Epoch 313,   100 Mini Batches] loss: 0.401\n",
      "[Epoch 313,   150 Mini Batches] loss: 0.387\n",
      "tensor(0.4123)\n",
      "[Epoch 314,    50 Mini Batches] loss: 0.407\n",
      "[Epoch 314,   100 Mini Batches] loss: 0.385\n",
      "[Epoch 314,   150 Mini Batches] loss: 0.388\n",
      "tensor(0.4107)\n",
      "[Epoch 315,    50 Mini Batches] loss: 0.394\n",
      "[Epoch 315,   100 Mini Batches] loss: 0.399\n",
      "[Epoch 315,   150 Mini Batches] loss: 0.392\n",
      "tensor(0.4165)\n",
      "[Epoch 316,    50 Mini Batches] loss: 0.403\n",
      "[Epoch 316,   100 Mini Batches] loss: 0.392\n",
      "[Epoch 316,   150 Mini Batches] loss: 0.382\n",
      "tensor(0.4163)\n",
      "[Epoch 317,    50 Mini Batches] loss: 0.395\n",
      "[Epoch 317,   100 Mini Batches] loss: 0.399\n",
      "[Epoch 317,   150 Mini Batches] loss: 0.385\n",
      "tensor(0.4137)\n",
      "[Epoch 318,    50 Mini Batches] loss: 0.392\n",
      "[Epoch 318,   100 Mini Batches] loss: 0.389\n",
      "[Epoch 318,   150 Mini Batches] loss: 0.415\n",
      "tensor(0.4156)\n",
      "[Epoch 319,    50 Mini Batches] loss: 0.381\n",
      "[Epoch 319,   100 Mini Batches] loss: 0.389\n",
      "[Epoch 319,   150 Mini Batches] loss: 0.400\n",
      "tensor(0.4187)\n",
      "[Epoch 320,    50 Mini Batches] loss: 0.385\n",
      "[Epoch 320,   100 Mini Batches] loss: 0.399\n",
      "[Epoch 320,   150 Mini Batches] loss: 0.397\n",
      "tensor(0.4138)\n",
      "[Epoch 321,    50 Mini Batches] loss: 0.395\n",
      "[Epoch 321,   100 Mini Batches] loss: 0.389\n",
      "[Epoch 321,   150 Mini Batches] loss: 0.382\n",
      "tensor(0.4167)\n",
      "[Epoch 322,    50 Mini Batches] loss: 0.378\n",
      "[Epoch 322,   100 Mini Batches] loss: 0.401\n",
      "[Epoch 322,   150 Mini Batches] loss: 0.382\n",
      "tensor(0.4139)\n",
      "[Epoch 323,    50 Mini Batches] loss: 0.397\n",
      "[Epoch 323,   100 Mini Batches] loss: 0.390\n",
      "[Epoch 323,   150 Mini Batches] loss: 0.388\n",
      "tensor(0.4137)\n",
      "[Epoch 324,    50 Mini Batches] loss: 0.406\n",
      "[Epoch 324,   100 Mini Batches] loss: 0.399\n",
      "[Epoch 324,   150 Mini Batches] loss: 0.375\n",
      "tensor(0.4132)\n",
      "[Epoch 325,    50 Mini Batches] loss: 0.387\n",
      "[Epoch 325,   100 Mini Batches] loss: 0.385\n",
      "[Epoch 325,   150 Mini Batches] loss: 0.394\n",
      "tensor(0.4232)\n",
      "[Epoch 326,    50 Mini Batches] loss: 0.382\n",
      "[Epoch 326,   100 Mini Batches] loss: 0.409\n",
      "[Epoch 326,   150 Mini Batches] loss: 0.390\n",
      "tensor(0.4095)\n",
      "[Epoch 327,    50 Mini Batches] loss: 0.380\n",
      "[Epoch 327,   100 Mini Batches] loss: 0.403\n",
      "[Epoch 327,   150 Mini Batches] loss: 0.393\n",
      "tensor(0.4139)\n",
      "[Epoch 328,    50 Mini Batches] loss: 0.381\n",
      "[Epoch 328,   100 Mini Batches] loss: 0.395\n",
      "[Epoch 328,   150 Mini Batches] loss: 0.389\n",
      "tensor(0.4121)\n",
      "[Epoch 329,    50 Mini Batches] loss: 0.387\n",
      "[Epoch 329,   100 Mini Batches] loss: 0.404\n",
      "[Epoch 329,   150 Mini Batches] loss: 0.389\n",
      "tensor(0.4130)\n",
      "[Epoch 330,    50 Mini Batches] loss: 0.390\n",
      "[Epoch 330,   100 Mini Batches] loss: 0.401\n",
      "[Epoch 330,   150 Mini Batches] loss: 0.387\n",
      "tensor(0.4179)\n",
      "[Epoch 331,    50 Mini Batches] loss: 0.394\n",
      "[Epoch 331,   100 Mini Batches] loss: 0.392\n",
      "[Epoch 331,   150 Mini Batches] loss: 0.399\n",
      "tensor(0.4136)\n",
      "[Epoch 332,    50 Mini Batches] loss: 0.389\n",
      "[Epoch 332,   100 Mini Batches] loss: 0.394\n",
      "[Epoch 332,   150 Mini Batches] loss: 0.395\n",
      "tensor(0.4157)\n",
      "[Epoch 333,    50 Mini Batches] loss: 0.402\n",
      "[Epoch 333,   100 Mini Batches] loss: 0.404\n",
      "[Epoch 333,   150 Mini Batches] loss: 0.379\n",
      "tensor(0.4118)\n",
      "[Epoch 334,    50 Mini Batches] loss: 0.400\n",
      "[Epoch 334,   100 Mini Batches] loss: 0.381\n",
      "[Epoch 334,   150 Mini Batches] loss: 0.392\n",
      "tensor(0.4157)\n",
      "[Epoch 335,    50 Mini Batches] loss: 0.394\n",
      "[Epoch 335,   100 Mini Batches] loss: 0.386\n",
      "[Epoch 335,   150 Mini Batches] loss: 0.397\n",
      "tensor(0.4133)\n",
      "[Epoch 336,    50 Mini Batches] loss: 0.403\n",
      "[Epoch 336,   100 Mini Batches] loss: 0.393\n",
      "[Epoch 336,   150 Mini Batches] loss: 0.400\n",
      "tensor(0.4153)\n",
      "[Epoch 337,    50 Mini Batches] loss: 0.388\n",
      "[Epoch 337,   100 Mini Batches] loss: 0.400\n",
      "[Epoch 337,   150 Mini Batches] loss: 0.394\n",
      "tensor(0.4142)\n",
      "[Epoch 338,    50 Mini Batches] loss: 0.395\n",
      "[Epoch 338,   100 Mini Batches] loss: 0.393\n",
      "[Epoch 338,   150 Mini Batches] loss: 0.396\n",
      "tensor(0.4147)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 339,    50 Mini Batches] loss: 0.396\n",
      "[Epoch 339,   100 Mini Batches] loss: 0.395\n",
      "[Epoch 339,   150 Mini Batches] loss: 0.396\n",
      "tensor(0.4144)\n",
      "[Epoch 340,    50 Mini Batches] loss: 0.389\n",
      "[Epoch 340,   100 Mini Batches] loss: 0.408\n",
      "[Epoch 340,   150 Mini Batches] loss: 0.396\n",
      "tensor(0.4121)\n",
      "[Epoch 341,    50 Mini Batches] loss: 0.404\n",
      "[Epoch 341,   100 Mini Batches] loss: 0.387\n",
      "[Epoch 341,   150 Mini Batches] loss: 0.399\n",
      "tensor(0.4116)\n",
      "[Epoch 342,    50 Mini Batches] loss: 0.394\n",
      "[Epoch 342,   100 Mini Batches] loss: 0.396\n",
      "[Epoch 342,   150 Mini Batches] loss: 0.395\n",
      "tensor(0.4134)\n",
      "[Epoch 343,    50 Mini Batches] loss: 0.377\n",
      "[Epoch 343,   100 Mini Batches] loss: 0.390\n",
      "[Epoch 343,   150 Mini Batches] loss: 0.413\n",
      "tensor(0.4142)\n",
      "[Epoch 344,    50 Mini Batches] loss: 0.389\n",
      "[Epoch 344,   100 Mini Batches] loss: 0.391\n",
      "[Epoch 344,   150 Mini Batches] loss: 0.397\n",
      "tensor(0.4152)\n",
      "[Epoch 345,    50 Mini Batches] loss: 0.391\n",
      "[Epoch 345,   100 Mini Batches] loss: 0.397\n",
      "[Epoch 345,   150 Mini Batches] loss: 0.387\n",
      "tensor(0.4139)\n",
      "[Epoch 346,    50 Mini Batches] loss: 0.397\n",
      "[Epoch 346,   100 Mini Batches] loss: 0.378\n",
      "[Epoch 346,   150 Mini Batches] loss: 0.404\n",
      "tensor(0.4138)\n",
      "[Epoch 347,    50 Mini Batches] loss: 0.377\n",
      "[Epoch 347,   100 Mini Batches] loss: 0.391\n",
      "[Epoch 347,   150 Mini Batches] loss: 0.410\n",
      "tensor(0.4146)\n",
      "[Epoch 348,    50 Mini Batches] loss: 0.381\n",
      "[Epoch 348,   100 Mini Batches] loss: 0.381\n",
      "[Epoch 348,   150 Mini Batches] loss: 0.412\n",
      "tensor(0.4209)\n",
      "[Epoch 349,    50 Mini Batches] loss: 0.389\n",
      "[Epoch 349,   100 Mini Batches] loss: 0.378\n",
      "[Epoch 349,   150 Mini Batches] loss: 0.404\n",
      "tensor(0.4131)\n",
      "[Epoch 350,    50 Mini Batches] loss: 0.387\n",
      "[Epoch 350,   100 Mini Batches] loss: 0.389\n",
      "[Epoch 350,   150 Mini Batches] loss: 0.397\n",
      "tensor(0.4136)\n",
      "[Epoch 351,    50 Mini Batches] loss: 0.400\n",
      "[Epoch 351,   100 Mini Batches] loss: 0.391\n",
      "[Epoch 351,   150 Mini Batches] loss: 0.390\n",
      "tensor(0.4110)\n",
      "[Epoch 352,    50 Mini Batches] loss: 0.375\n",
      "[Epoch 352,   100 Mini Batches] loss: 0.392\n",
      "[Epoch 352,   150 Mini Batches] loss: 0.405\n",
      "tensor(0.4140)\n",
      "[Epoch 353,    50 Mini Batches] loss: 0.398\n",
      "[Epoch 353,   100 Mini Batches] loss: 0.394\n",
      "[Epoch 353,   150 Mini Batches] loss: 0.390\n",
      "tensor(0.4134)\n",
      "[Epoch 354,    50 Mini Batches] loss: 0.397\n",
      "[Epoch 354,   100 Mini Batches] loss: 0.408\n",
      "[Epoch 354,   150 Mini Batches] loss: 0.388\n",
      "tensor(0.4165)\n",
      "[Epoch 355,    50 Mini Batches] loss: 0.388\n",
      "[Epoch 355,   100 Mini Batches] loss: 0.393\n",
      "[Epoch 355,   150 Mini Batches] loss: 0.387\n",
      "tensor(0.4108)\n",
      "[Epoch 356,    50 Mini Batches] loss: 0.378\n",
      "[Epoch 356,   100 Mini Batches] loss: 0.402\n",
      "[Epoch 356,   150 Mini Batches] loss: 0.392\n",
      "tensor(0.4122)\n",
      "[Epoch 357,    50 Mini Batches] loss: 0.383\n",
      "[Epoch 357,   100 Mini Batches] loss: 0.390\n",
      "[Epoch 357,   150 Mini Batches] loss: 0.383\n",
      "tensor(0.4132)\n",
      "[Epoch 358,    50 Mini Batches] loss: 0.406\n",
      "[Epoch 358,   100 Mini Batches] loss: 0.386\n",
      "[Epoch 358,   150 Mini Batches] loss: 0.386\n",
      "tensor(0.4176)\n",
      "[Epoch 359,    50 Mini Batches] loss: 0.378\n",
      "[Epoch 359,   100 Mini Batches] loss: 0.393\n",
      "[Epoch 359,   150 Mini Batches] loss: 0.393\n",
      "tensor(0.4127)\n",
      "[Epoch 360,    50 Mini Batches] loss: 0.392\n",
      "[Epoch 360,   100 Mini Batches] loss: 0.389\n",
      "[Epoch 360,   150 Mini Batches] loss: 0.376\n",
      "tensor(0.4125)\n",
      "[Epoch 361,    50 Mini Batches] loss: 0.390\n",
      "[Epoch 361,   100 Mini Batches] loss: 0.407\n",
      "[Epoch 361,   150 Mini Batches] loss: 0.394\n",
      "tensor(0.4162)\n",
      "[Epoch 362,    50 Mini Batches] loss: 0.385\n",
      "[Epoch 362,   100 Mini Batches] loss: 0.403\n",
      "[Epoch 362,   150 Mini Batches] loss: 0.399\n",
      "tensor(0.4144)\n",
      "[Epoch 363,    50 Mini Batches] loss: 0.386\n",
      "[Epoch 363,   100 Mini Batches] loss: 0.411\n",
      "[Epoch 363,   150 Mini Batches] loss: 0.376\n",
      "tensor(0.4138)\n",
      "[Epoch 364,    50 Mini Batches] loss: 0.382\n",
      "[Epoch 364,   100 Mini Batches] loss: 0.399\n",
      "[Epoch 364,   150 Mini Batches] loss: 0.390\n",
      "tensor(0.4159)\n",
      "[Epoch 365,    50 Mini Batches] loss: 0.375\n",
      "[Epoch 365,   100 Mini Batches] loss: 0.389\n",
      "[Epoch 365,   150 Mini Batches] loss: 0.398\n",
      "tensor(0.4141)\n",
      "[Epoch 366,    50 Mini Batches] loss: 0.400\n",
      "[Epoch 366,   100 Mini Batches] loss: 0.381\n",
      "[Epoch 366,   150 Mini Batches] loss: 0.391\n",
      "tensor(0.4143)\n",
      "[Epoch 367,    50 Mini Batches] loss: 0.370\n",
      "[Epoch 367,   100 Mini Batches] loss: 0.397\n",
      "[Epoch 367,   150 Mini Batches] loss: 0.397\n",
      "tensor(0.4175)\n",
      "[Epoch 368,    50 Mini Batches] loss: 0.397\n",
      "[Epoch 368,   100 Mini Batches] loss: 0.387\n",
      "[Epoch 368,   150 Mini Batches] loss: 0.390\n",
      "tensor(0.4145)\n",
      "[Epoch 369,    50 Mini Batches] loss: 0.402\n",
      "[Epoch 369,   100 Mini Batches] loss: 0.401\n",
      "[Epoch 369,   150 Mini Batches] loss: 0.383\n",
      "tensor(0.4200)\n",
      "[Epoch 370,    50 Mini Batches] loss: 0.386\n",
      "[Epoch 370,   100 Mini Batches] loss: 0.385\n",
      "[Epoch 370,   150 Mini Batches] loss: 0.393\n",
      "tensor(0.4155)\n",
      "[Epoch 371,    50 Mini Batches] loss: 0.387\n",
      "[Epoch 371,   100 Mini Batches] loss: 0.399\n",
      "[Epoch 371,   150 Mini Batches] loss: 0.396\n",
      "tensor(0.4139)\n",
      "[Epoch 372,    50 Mini Batches] loss: 0.386\n",
      "[Epoch 372,   100 Mini Batches] loss: 0.405\n",
      "[Epoch 372,   150 Mini Batches] loss: 0.382\n",
      "tensor(0.4154)\n",
      "[Epoch 373,    50 Mini Batches] loss: 0.385\n",
      "[Epoch 373,   100 Mini Batches] loss: 0.388\n",
      "[Epoch 373,   150 Mini Batches] loss: 0.403\n",
      "tensor(0.4123)\n",
      "[Epoch 374,    50 Mini Batches] loss: 0.382\n",
      "[Epoch 374,   100 Mini Batches] loss: 0.402\n",
      "[Epoch 374,   150 Mini Batches] loss: 0.387\n",
      "tensor(0.4184)\n",
      "[Epoch 375,    50 Mini Batches] loss: 0.386\n",
      "[Epoch 375,   100 Mini Batches] loss: 0.400\n",
      "[Epoch 375,   150 Mini Batches] loss: 0.400\n",
      "tensor(0.4150)\n",
      "[Epoch 376,    50 Mini Batches] loss: 0.387\n",
      "[Epoch 376,   100 Mini Batches] loss: 0.390\n",
      "[Epoch 376,   150 Mini Batches] loss: 0.401\n",
      "tensor(0.4163)\n",
      "[Epoch 377,    50 Mini Batches] loss: 0.382\n",
      "[Epoch 377,   100 Mini Batches] loss: 0.393\n",
      "[Epoch 377,   150 Mini Batches] loss: 0.385\n",
      "tensor(0.4166)\n",
      "[Epoch 378,    50 Mini Batches] loss: 0.399\n",
      "[Epoch 378,   100 Mini Batches] loss: 0.393\n",
      "[Epoch 378,   150 Mini Batches] loss: 0.386\n",
      "tensor(0.4149)\n",
      "[Epoch 379,    50 Mini Batches] loss: 0.395\n",
      "[Epoch 379,   100 Mini Batches] loss: 0.396\n",
      "[Epoch 379,   150 Mini Batches] loss: 0.392\n",
      "tensor(0.4149)\n",
      "[Epoch 380,    50 Mini Batches] loss: 0.397\n",
      "[Epoch 380,   100 Mini Batches] loss: 0.388\n",
      "[Epoch 380,   150 Mini Batches] loss: 0.397\n",
      "tensor(0.4190)\n",
      "[Epoch 381,    50 Mini Batches] loss: 0.394\n",
      "[Epoch 381,   100 Mini Batches] loss: 0.378\n",
      "[Epoch 381,   150 Mini Batches] loss: 0.411\n",
      "tensor(0.4133)\n",
      "[Epoch 382,    50 Mini Batches] loss: 0.383\n",
      "[Epoch 382,   100 Mini Batches] loss: 0.396\n",
      "[Epoch 382,   150 Mini Batches] loss: 0.387\n",
      "tensor(0.4146)\n",
      "[Epoch 383,    50 Mini Batches] loss: 0.389\n",
      "[Epoch 383,   100 Mini Batches] loss: 0.390\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-4c7aaa9735fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-152-ecf2ab0de759>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m#print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6588d677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6470, 50, 50) (6470,)\n",
      "1164\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_set = ImageDataset()\n",
    "print(len(train_set))\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8594b184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test model against training data\n",
    "\n",
    "model = torch.load(\"ClassifyNet.pth\")\n",
    "\n",
    "predictions = np.zeros((len(test_set)))\n",
    "\n",
    "output = []\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(test_loader):\n",
    "        image = data[0].float().to(device)\n",
    "        classify_output = model(image)\n",
    "    \n",
    "        y_pred = (torch.reshape(classify_output.cpu(), (-1,)) > 0.5).numpy()\n",
    "        \n",
    "        predictions[idx] = y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d431cc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 0., 0., 1.])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
