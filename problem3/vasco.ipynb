{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6484c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from sklearn.metrics import balanced_accuracy_score as BACC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import eval_scores as eval\n",
    "import cv2 as cv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5ee13ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(xtrain, ytrain):\n",
    "    print(xtrain.shape, ytrain.shape)\n",
    "    #(6470, 50, 50) (6470,)\n",
    "    #(50, 50) ()\n",
    "    xtrain_len = len(xtrain)\n",
    "    \n",
    "    aug_xtrain = np.zeros((xtrain_len*2, 50, 50))\n",
    "    aug_ytrain = np.zeros((xtrain_len*2))\n",
    "    \n",
    "    aug_xtrain[0:xtrain_len, :, :] = xtrain\n",
    "    aug_ytrain[0:xtrain_len] = ytrain\n",
    "    \n",
    "    for idx in range(xtrain_len):\n",
    "        image = xtrain[idx,:,:]\n",
    "        label = ytrain[idx]\n",
    "                \n",
    "        angle = int(random.uniform(-90, 90))\n",
    "        h, w = image.shape[0], image.shape[1]\n",
    "        M = cv.getRotationMatrix2D((int(w/2), int(h/2)), angle, 1)\n",
    "        rotated = cv.warpAffine(image, M, (w, h))\n",
    "        \n",
    "        flipped = cv.flip(rotated, 1)\n",
    "\n",
    "\n",
    "        aug_xtrain[xtrain_len+idx] = flipped\n",
    "        aug_ytrain[xtrain_len+idx] = label\n",
    "        \n",
    "        \n",
    "    return aug_xtrain, aug_ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc81ba2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6470, 50, 50) (6470,)\n"
     ]
    }
   ],
   "source": [
    "xtrain = np.load(\"Xtrain_Classification_Part1.npy\")\n",
    "ytrain = np.load(\"Ytrain_Classification_Part1.npy\")\n",
    "xtrain_len = len(xtrain)\n",
    "ytrain_len = len(ytrain)\n",
    "\n",
    "#Reshape Images\n",
    "xtrain = xtrain.reshape((xtrain_len,50,50))\n",
    "mean = xtrain.mean(axis=(0, 1, 2)) \n",
    "std = xtrain.std(axis=(0, 1, 2))\n",
    "\n",
    "xtrain = (xtrain - mean)/std  \n",
    "\n",
    "xtrain, ytrain = augment_data(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cc4d8a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xtrain[6479,:,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dab60c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        xtrain = np.load(\"Xtrain_Classification_Part1.npy\")\n",
    "        ytrain = np.load(\"Ytrain_Classification_Part1.npy\")\n",
    "        xtest = np.load(\"Xtest_Classification_Part1.npy\")\n",
    "\n",
    "\n",
    "        xtrain_len = len(xtrain)\n",
    "        ytrain_len = len(ytrain)\n",
    "        xtest_len = len(xtest)\n",
    "\n",
    "        #Reshape Images\n",
    "        xtrain = xtrain.reshape((xtrain_len,50,50))\n",
    "        mean = xtrain.mean(axis=(0, 1, 2)) \n",
    "        std = xtrain.std(axis=(0, 1, 2))\n",
    "\n",
    "        xtrain = (xtrain - mean)/std  \n",
    "        \n",
    "        xtrain, ytrain = augment_data(xtrain, ytrain)\n",
    "        \n",
    "        new_xtrain_len = len(xtrain)\n",
    "        new_ytrain_len = len(ytrain)\n",
    "        \n",
    "        self.xtrain = xtrain.reshape((new_xtrain_len,1,50,50))\n",
    "        self.xtest = xtest.reshape((xtest_len,1,50,50))\n",
    "\n",
    "        self.ytrain = ytrain.reshape(new_ytrain_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xtrain)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #mage = self.xtrain[idx,:,:,:].reshape((50,50))\n",
    "        #image = cv.resize(image, (25,25))\n",
    "        #image = image.reshape(1,25,25)\n",
    "        \n",
    "        image = self.xtrain[idx, :, :, :]\n",
    "        label = self.ytrain[idx]\n",
    "        \n",
    "\n",
    "        return image, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c6ff07e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ClassifyNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10,\n",
    "                              kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20,\n",
    "                              kernel_size=5, stride=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(in_features=1620, out_features=800)\n",
    "        self.fc2 = nn.Linear(in_features=1620, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.fc1(x)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "301b424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size' : 64,\n",
    "    'lr' : 0.001,\n",
    "    'momentum' : 0.9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ca190a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    \n",
    "    num_epochs = 400\n",
    "    best_score = 1.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        for idx, data in enumerate(train_loader):\n",
    "            image, label = data[0].float().to(device), data[1].float()\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            classify_output = model(image)\n",
    "            \n",
    "            \n",
    "            y_true = torch.reshape(label.cpu(), (-1,))\n",
    "            y_pred = torch.reshape(classify_output.cpu(), (-1,))\n",
    "            \n",
    "            loss = criterion(y_pred, y_true)\n",
    "\n",
    "            #print(loss)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print for mini batches\n",
    "            running_loss += loss.item()\n",
    "            if idx % 50 == 49:  # every 50 mini batches\n",
    "                print('[Epoch %d, %5d Mini Batches] loss: %.3f' %\n",
    "                      (epoch + 1, idx + 1, running_loss/50))\n",
    "                running_loss = 0.0\n",
    "               \n",
    "        model.eval()\n",
    "        test_score = 0.0\n",
    "        \n",
    "         \n",
    "        with torch.no_grad():\n",
    "            for idx, data in enumerate(test_loader):\n",
    "                image, label = data[0].float().to(device), data[1].float()\n",
    "\n",
    "                classify_output = model(image)\n",
    "\n",
    "                y_true = torch.reshape(label.cpu(), (-1,))\n",
    "                y_pred = torch.reshape(classify_output.cpu(), (-1,))\n",
    "\n",
    "                test_loss = criterion(y_pred, y_true)\n",
    "\n",
    "                test_score += test_loss\n",
    "\n",
    "            test_score /= len(test_loader)\n",
    "\n",
    "            print(test_score)\n",
    "\n",
    "            if test_score < best_score:\n",
    "                torch.save(model, \"ClassifyNet.pth\")\n",
    "                best_score = test_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bc1d7acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6470, 50, 50) (6470,)\n"
     ]
    }
   ],
   "source": [
    "data_set = ImageDataset()\n",
    "\n",
    "train_set, test_set = train_test_split(data_set, test_size=0.2, random_state=1, shuffle=True)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=config['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=config['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bf166f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4a11251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device= torch.device('cpu')\n",
    "model = ClassifyNet().to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=config['momentum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862bd8da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1,    50 Mini Batches] loss: 0.681\n",
      "[Epoch 1,   100 Mini Batches] loss: 0.660\n",
      "[Epoch 1,   150 Mini Batches] loss: 0.638\n",
      "tensor(0.6084)\n",
      "[Epoch 2,    50 Mini Batches] loss: 0.611\n",
      "[Epoch 2,   100 Mini Batches] loss: 0.596\n",
      "[Epoch 2,   150 Mini Batches] loss: 0.591\n",
      "tensor(0.5662)\n",
      "[Epoch 3,    50 Mini Batches] loss: 0.576\n",
      "[Epoch 3,   100 Mini Batches] loss: 0.582\n",
      "[Epoch 3,   150 Mini Batches] loss: 0.574\n",
      "tensor(0.5470)\n",
      "[Epoch 4,    50 Mini Batches] loss: 0.568\n",
      "[Epoch 4,   100 Mini Batches] loss: 0.556\n",
      "[Epoch 4,   150 Mini Batches] loss: 0.564\n",
      "tensor(0.5525)\n",
      "[Epoch 5,    50 Mini Batches] loss: 0.563\n",
      "[Epoch 5,   100 Mini Batches] loss: 0.550\n",
      "[Epoch 5,   150 Mini Batches] loss: 0.548\n",
      "tensor(0.5199)\n",
      "[Epoch 6,    50 Mini Batches] loss: 0.546\n",
      "[Epoch 6,   100 Mini Batches] loss: 0.532\n",
      "[Epoch 6,   150 Mini Batches] loss: 0.536\n",
      "tensor(0.5117)\n",
      "[Epoch 7,    50 Mini Batches] loss: 0.521\n",
      "[Epoch 7,   100 Mini Batches] loss: 0.530\n",
      "[Epoch 7,   150 Mini Batches] loss: 0.539\n",
      "tensor(0.5013)\n",
      "[Epoch 8,    50 Mini Batches] loss: 0.521\n",
      "[Epoch 8,   100 Mini Batches] loss: 0.529\n",
      "[Epoch 8,   150 Mini Batches] loss: 0.518\n",
      "tensor(0.4920)\n",
      "[Epoch 9,    50 Mini Batches] loss: 0.510\n",
      "[Epoch 9,   100 Mini Batches] loss: 0.530\n",
      "[Epoch 9,   150 Mini Batches] loss: 0.525\n",
      "tensor(0.4893)\n",
      "[Epoch 10,    50 Mini Batches] loss: 0.517\n",
      "[Epoch 10,   100 Mini Batches] loss: 0.522\n",
      "[Epoch 10,   150 Mini Batches] loss: 0.505\n",
      "tensor(0.4865)\n",
      "[Epoch 11,    50 Mini Batches] loss: 0.498\n",
      "[Epoch 11,   100 Mini Batches] loss: 0.516\n",
      "[Epoch 11,   150 Mini Batches] loss: 0.508\n",
      "tensor(0.4776)\n",
      "[Epoch 12,    50 Mini Batches] loss: 0.505\n",
      "[Epoch 12,   100 Mini Batches] loss: 0.497\n",
      "[Epoch 12,   150 Mini Batches] loss: 0.492\n",
      "tensor(0.4711)\n",
      "[Epoch 13,    50 Mini Batches] loss: 0.492\n",
      "[Epoch 13,   100 Mini Batches] loss: 0.512\n",
      "[Epoch 13,   150 Mini Batches] loss: 0.488\n",
      "tensor(0.4679)\n",
      "[Epoch 14,    50 Mini Batches] loss: 0.487\n",
      "[Epoch 14,   100 Mini Batches] loss: 0.490\n",
      "[Epoch 14,   150 Mini Batches] loss: 0.487\n",
      "tensor(0.4605)\n",
      "[Epoch 15,    50 Mini Batches] loss: 0.485\n",
      "[Epoch 15,   100 Mini Batches] loss: 0.488\n",
      "[Epoch 15,   150 Mini Batches] loss: 0.475\n",
      "tensor(0.4555)\n",
      "[Epoch 16,    50 Mini Batches] loss: 0.470\n",
      "[Epoch 16,   100 Mini Batches] loss: 0.488\n",
      "[Epoch 16,   150 Mini Batches] loss: 0.485\n",
      "tensor(0.4589)\n",
      "[Epoch 17,    50 Mini Batches] loss: 0.480\n",
      "[Epoch 17,   100 Mini Batches] loss: 0.466\n",
      "[Epoch 17,   150 Mini Batches] loss: 0.480\n",
      "tensor(0.4444)\n",
      "[Epoch 18,    50 Mini Batches] loss: 0.473\n",
      "[Epoch 18,   100 Mini Batches] loss: 0.477\n",
      "[Epoch 18,   150 Mini Batches] loss: 0.464\n",
      "tensor(0.4490)\n",
      "[Epoch 19,    50 Mini Batches] loss: 0.459\n",
      "[Epoch 19,   100 Mini Batches] loss: 0.471\n",
      "[Epoch 19,   150 Mini Batches] loss: 0.472\n",
      "tensor(0.4426)\n",
      "[Epoch 20,    50 Mini Batches] loss: 0.462\n",
      "[Epoch 20,   100 Mini Batches] loss: 0.461\n",
      "[Epoch 20,   150 Mini Batches] loss: 0.462\n",
      "tensor(0.4414)\n",
      "[Epoch 21,    50 Mini Batches] loss: 0.455\n",
      "[Epoch 21,   100 Mini Batches] loss: 0.446\n",
      "[Epoch 21,   150 Mini Batches] loss: 0.458\n",
      "tensor(0.4365)\n",
      "[Epoch 22,    50 Mini Batches] loss: 0.455\n",
      "[Epoch 22,   100 Mini Batches] loss: 0.446\n",
      "[Epoch 22,   150 Mini Batches] loss: 0.453\n",
      "tensor(0.4430)\n",
      "[Epoch 23,    50 Mini Batches] loss: 0.444\n",
      "[Epoch 23,   100 Mini Batches] loss: 0.451\n",
      "[Epoch 23,   150 Mini Batches] loss: 0.443\n",
      "tensor(0.4265)\n",
      "[Epoch 24,    50 Mini Batches] loss: 0.442\n",
      "[Epoch 24,   100 Mini Batches] loss: 0.452\n",
      "[Epoch 24,   150 Mini Batches] loss: 0.435\n",
      "tensor(0.4213)\n",
      "[Epoch 25,    50 Mini Batches] loss: 0.434\n",
      "[Epoch 25,   100 Mini Batches] loss: 0.440\n",
      "[Epoch 25,   150 Mini Batches] loss: 0.444\n",
      "tensor(0.4246)\n",
      "[Epoch 26,    50 Mini Batches] loss: 0.433\n",
      "[Epoch 26,   100 Mini Batches] loss: 0.436\n",
      "[Epoch 26,   150 Mini Batches] loss: 0.443\n",
      "tensor(0.4190)\n",
      "[Epoch 27,    50 Mini Batches] loss: 0.431\n",
      "[Epoch 27,   100 Mini Batches] loss: 0.443\n",
      "[Epoch 27,   150 Mini Batches] loss: 0.440\n",
      "tensor(0.4141)\n",
      "[Epoch 28,    50 Mini Batches] loss: 0.420\n",
      "[Epoch 28,   100 Mini Batches] loss: 0.428\n",
      "[Epoch 28,   150 Mini Batches] loss: 0.434\n",
      "tensor(0.4088)\n",
      "[Epoch 29,    50 Mini Batches] loss: 0.417\n",
      "[Epoch 29,   100 Mini Batches] loss: 0.431\n",
      "[Epoch 29,   150 Mini Batches] loss: 0.425\n",
      "tensor(0.4077)\n",
      "[Epoch 30,    50 Mini Batches] loss: 0.420\n",
      "[Epoch 30,   100 Mini Batches] loss: 0.428\n",
      "[Epoch 30,   150 Mini Batches] loss: 0.414\n",
      "tensor(0.4095)\n",
      "[Epoch 31,    50 Mini Batches] loss: 0.424\n",
      "[Epoch 31,   100 Mini Batches] loss: 0.414\n",
      "[Epoch 31,   150 Mini Batches] loss: 0.418\n",
      "tensor(0.4017)\n",
      "[Epoch 32,    50 Mini Batches] loss: 0.418\n",
      "[Epoch 32,   100 Mini Batches] loss: 0.415\n",
      "[Epoch 32,   150 Mini Batches] loss: 0.401\n",
      "tensor(0.4014)\n",
      "[Epoch 33,    50 Mini Batches] loss: 0.405\n",
      "[Epoch 33,   100 Mini Batches] loss: 0.414\n",
      "[Epoch 33,   150 Mini Batches] loss: 0.415\n",
      "tensor(0.4019)\n",
      "[Epoch 34,    50 Mini Batches] loss: 0.416\n",
      "[Epoch 34,   100 Mini Batches] loss: 0.418\n",
      "[Epoch 34,   150 Mini Batches] loss: 0.403\n",
      "tensor(0.3966)\n",
      "[Epoch 35,    50 Mini Batches] loss: 0.403\n",
      "[Epoch 35,   100 Mini Batches] loss: 0.394\n",
      "[Epoch 35,   150 Mini Batches] loss: 0.416\n",
      "tensor(0.3964)\n",
      "[Epoch 36,    50 Mini Batches] loss: 0.408\n",
      "[Epoch 36,   100 Mini Batches] loss: 0.400\n",
      "[Epoch 36,   150 Mini Batches] loss: 0.400\n",
      "tensor(0.3963)\n",
      "[Epoch 37,    50 Mini Batches] loss: 0.408\n",
      "[Epoch 37,   100 Mini Batches] loss: 0.403\n",
      "[Epoch 37,   150 Mini Batches] loss: 0.381\n",
      "tensor(0.3970)\n",
      "[Epoch 38,    50 Mini Batches] loss: 0.399\n",
      "[Epoch 38,   100 Mini Batches] loss: 0.387\n",
      "[Epoch 38,   150 Mini Batches] loss: 0.401\n",
      "tensor(0.3874)\n",
      "[Epoch 39,    50 Mini Batches] loss: 0.387\n",
      "[Epoch 39,   100 Mini Batches] loss: 0.378\n",
      "[Epoch 39,   150 Mini Batches] loss: 0.406\n",
      "tensor(0.3902)\n",
      "[Epoch 40,    50 Mini Batches] loss: 0.390\n",
      "[Epoch 40,   100 Mini Batches] loss: 0.388\n",
      "[Epoch 40,   150 Mini Batches] loss: 0.393\n",
      "tensor(0.3994)\n",
      "[Epoch 41,    50 Mini Batches] loss: 0.378\n",
      "[Epoch 41,   100 Mini Batches] loss: 0.395\n",
      "[Epoch 41,   150 Mini Batches] loss: 0.383\n",
      "tensor(0.3853)\n",
      "[Epoch 42,    50 Mini Batches] loss: 0.382\n",
      "[Epoch 42,   100 Mini Batches] loss: 0.374\n",
      "[Epoch 42,   150 Mini Batches] loss: 0.397\n",
      "tensor(0.3844)\n",
      "[Epoch 43,    50 Mini Batches] loss: 0.380\n",
      "[Epoch 43,   100 Mini Batches] loss: 0.388\n",
      "[Epoch 43,   150 Mini Batches] loss: 0.377\n",
      "tensor(0.3846)\n",
      "[Epoch 44,    50 Mini Batches] loss: 0.393\n",
      "[Epoch 44,   100 Mini Batches] loss: 0.374\n",
      "[Epoch 44,   150 Mini Batches] loss: 0.387\n",
      "tensor(0.3788)\n",
      "[Epoch 45,    50 Mini Batches] loss: 0.372\n",
      "[Epoch 45,   100 Mini Batches] loss: 0.370\n",
      "[Epoch 45,   150 Mini Batches] loss: 0.390\n",
      "tensor(0.3781)\n",
      "[Epoch 46,    50 Mini Batches] loss: 0.384\n",
      "[Epoch 46,   100 Mini Batches] loss: 0.376\n",
      "[Epoch 46,   150 Mini Batches] loss: 0.377\n",
      "tensor(0.3770)\n",
      "[Epoch 47,    50 Mini Batches] loss: 0.377\n",
      "[Epoch 47,   100 Mini Batches] loss: 0.377\n",
      "[Epoch 47,   150 Mini Batches] loss: 0.377\n",
      "tensor(0.3806)\n",
      "[Epoch 48,    50 Mini Batches] loss: 0.376\n",
      "[Epoch 48,   100 Mini Batches] loss: 0.378\n",
      "[Epoch 48,   150 Mini Batches] loss: 0.373\n",
      "tensor(0.3747)\n",
      "[Epoch 49,    50 Mini Batches] loss: 0.371\n",
      "[Epoch 49,   100 Mini Batches] loss: 0.373\n",
      "[Epoch 49,   150 Mini Batches] loss: 0.372\n",
      "tensor(0.3738)\n",
      "[Epoch 50,    50 Mini Batches] loss: 0.369\n",
      "[Epoch 50,   100 Mini Batches] loss: 0.362\n",
      "[Epoch 50,   150 Mini Batches] loss: 0.375\n",
      "tensor(0.3718)\n",
      "[Epoch 51,    50 Mini Batches] loss: 0.376\n",
      "[Epoch 51,   100 Mini Batches] loss: 0.362\n",
      "[Epoch 51,   150 Mini Batches] loss: 0.372\n",
      "tensor(0.3710)\n",
      "[Epoch 52,    50 Mini Batches] loss: 0.366\n",
      "[Epoch 52,   100 Mini Batches] loss: 0.351\n",
      "[Epoch 52,   150 Mini Batches] loss: 0.376\n",
      "tensor(0.3694)\n",
      "[Epoch 53,    50 Mini Batches] loss: 0.358\n",
      "[Epoch 53,   100 Mini Batches] loss: 0.354\n",
      "[Epoch 53,   150 Mini Batches] loss: 0.364\n",
      "tensor(0.3647)\n",
      "[Epoch 54,    50 Mini Batches] loss: 0.366\n",
      "[Epoch 54,   100 Mini Batches] loss: 0.358\n",
      "[Epoch 54,   150 Mini Batches] loss: 0.357\n",
      "tensor(0.3655)\n",
      "[Epoch 55,    50 Mini Batches] loss: 0.350\n",
      "[Epoch 55,   100 Mini Batches] loss: 0.355\n",
      "[Epoch 55,   150 Mini Batches] loss: 0.368\n",
      "tensor(0.3649)\n",
      "[Epoch 56,    50 Mini Batches] loss: 0.351\n",
      "[Epoch 56,   100 Mini Batches] loss: 0.351\n",
      "[Epoch 56,   150 Mini Batches] loss: 0.363\n",
      "tensor(0.3703)\n",
      "[Epoch 57,    50 Mini Batches] loss: 0.360\n",
      "[Epoch 57,   100 Mini Batches] loss: 0.347\n",
      "[Epoch 57,   150 Mini Batches] loss: 0.356\n",
      "tensor(0.3616)\n",
      "[Epoch 58,    50 Mini Batches] loss: 0.348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 58,   100 Mini Batches] loss: 0.347\n",
      "[Epoch 58,   150 Mini Batches] loss: 0.352\n",
      "tensor(0.3603)\n",
      "[Epoch 59,    50 Mini Batches] loss: 0.344\n",
      "[Epoch 59,   100 Mini Batches] loss: 0.356\n",
      "[Epoch 59,   150 Mini Batches] loss: 0.353\n",
      "tensor(0.3617)\n",
      "[Epoch 60,    50 Mini Batches] loss: 0.342\n",
      "[Epoch 60,   100 Mini Batches] loss: 0.356\n",
      "[Epoch 60,   150 Mini Batches] loss: 0.356\n",
      "tensor(0.3641)\n",
      "[Epoch 61,    50 Mini Batches] loss: 0.347\n",
      "[Epoch 61,   100 Mini Batches] loss: 0.346\n",
      "[Epoch 61,   150 Mini Batches] loss: 0.347\n",
      "tensor(0.3584)\n",
      "[Epoch 62,    50 Mini Batches] loss: 0.348\n",
      "[Epoch 62,   100 Mini Batches] loss: 0.351\n",
      "[Epoch 62,   150 Mini Batches] loss: 0.341\n",
      "tensor(0.3694)\n",
      "[Epoch 63,    50 Mini Batches] loss: 0.366\n",
      "[Epoch 63,   100 Mini Batches] loss: 0.341\n",
      "[Epoch 63,   150 Mini Batches] loss: 0.342\n",
      "tensor(0.3594)\n",
      "[Epoch 64,    50 Mini Batches] loss: 0.342\n",
      "[Epoch 64,   100 Mini Batches] loss: 0.344\n",
      "[Epoch 64,   150 Mini Batches] loss: 0.337\n",
      "tensor(0.3633)\n",
      "[Epoch 65,    50 Mini Batches] loss: 0.344\n",
      "[Epoch 65,   100 Mini Batches] loss: 0.334\n",
      "[Epoch 65,   150 Mini Batches] loss: 0.335\n",
      "tensor(0.3595)\n",
      "[Epoch 66,    50 Mini Batches] loss: 0.324\n",
      "[Epoch 66,   100 Mini Batches] loss: 0.349\n",
      "[Epoch 66,   150 Mini Batches] loss: 0.355\n",
      "tensor(0.3585)\n",
      "[Epoch 67,    50 Mini Batches] loss: 0.338\n",
      "[Epoch 67,   100 Mini Batches] loss: 0.342\n",
      "[Epoch 67,   150 Mini Batches] loss: 0.323\n",
      "tensor(0.3544)\n",
      "[Epoch 68,    50 Mini Batches] loss: 0.346\n",
      "[Epoch 68,   100 Mini Batches] loss: 0.343\n",
      "[Epoch 68,   150 Mini Batches] loss: 0.336\n",
      "tensor(0.3548)\n",
      "[Epoch 69,    50 Mini Batches] loss: 0.344\n",
      "[Epoch 69,   100 Mini Batches] loss: 0.325\n",
      "[Epoch 69,   150 Mini Batches] loss: 0.348\n",
      "tensor(0.3587)\n",
      "[Epoch 70,    50 Mini Batches] loss: 0.333\n",
      "[Epoch 70,   100 Mini Batches] loss: 0.329\n",
      "[Epoch 70,   150 Mini Batches] loss: 0.340\n",
      "tensor(0.3548)\n",
      "[Epoch 71,    50 Mini Batches] loss: 0.326\n",
      "[Epoch 71,   100 Mini Batches] loss: 0.342\n",
      "[Epoch 71,   150 Mini Batches] loss: 0.326\n",
      "tensor(0.3548)\n",
      "[Epoch 72,    50 Mini Batches] loss: 0.332\n",
      "[Epoch 72,   100 Mini Batches] loss: 0.332\n",
      "[Epoch 72,   150 Mini Batches] loss: 0.332\n",
      "tensor(0.3612)\n",
      "[Epoch 73,    50 Mini Batches] loss: 0.334\n",
      "[Epoch 73,   100 Mini Batches] loss: 0.320\n",
      "[Epoch 73,   150 Mini Batches] loss: 0.346\n",
      "tensor(0.3523)\n",
      "[Epoch 74,    50 Mini Batches] loss: 0.324\n",
      "[Epoch 74,   100 Mini Batches] loss: 0.335\n",
      "[Epoch 74,   150 Mini Batches] loss: 0.341\n",
      "tensor(0.3503)\n",
      "[Epoch 75,    50 Mini Batches] loss: 0.318\n",
      "[Epoch 75,   100 Mini Batches] loss: 0.323\n",
      "[Epoch 75,   150 Mini Batches] loss: 0.324\n",
      "tensor(0.3637)\n",
      "[Epoch 76,    50 Mini Batches] loss: 0.314\n",
      "[Epoch 76,   100 Mini Batches] loss: 0.321\n",
      "[Epoch 76,   150 Mini Batches] loss: 0.333\n",
      "tensor(0.3569)\n",
      "[Epoch 77,    50 Mini Batches] loss: 0.330\n",
      "[Epoch 77,   100 Mini Batches] loss: 0.323\n",
      "[Epoch 77,   150 Mini Batches] loss: 0.319\n",
      "tensor(0.3520)\n",
      "[Epoch 78,    50 Mini Batches] loss: 0.313\n",
      "[Epoch 78,   100 Mini Batches] loss: 0.313\n",
      "[Epoch 78,   150 Mini Batches] loss: 0.330\n",
      "tensor(0.3528)\n",
      "[Epoch 79,    50 Mini Batches] loss: 0.317\n",
      "[Epoch 79,   100 Mini Batches] loss: 0.325\n",
      "[Epoch 79,   150 Mini Batches] loss: 0.329\n",
      "tensor(0.3550)\n",
      "[Epoch 80,    50 Mini Batches] loss: 0.328\n",
      "[Epoch 80,   100 Mini Batches] loss: 0.323\n",
      "[Epoch 80,   150 Mini Batches] loss: 0.309\n",
      "tensor(0.3534)\n",
      "[Epoch 81,    50 Mini Batches] loss: 0.297\n",
      "[Epoch 81,   100 Mini Batches] loss: 0.321\n",
      "[Epoch 81,   150 Mini Batches] loss: 0.329\n",
      "tensor(0.3477)\n",
      "[Epoch 82,    50 Mini Batches] loss: 0.314\n",
      "[Epoch 82,   100 Mini Batches] loss: 0.318\n",
      "[Epoch 82,   150 Mini Batches] loss: 0.308\n",
      "tensor(0.3512)\n",
      "[Epoch 83,    50 Mini Batches] loss: 0.314\n",
      "[Epoch 83,   100 Mini Batches] loss: 0.306\n",
      "[Epoch 83,   150 Mini Batches] loss: 0.328\n",
      "tensor(0.3451)\n",
      "[Epoch 84,    50 Mini Batches] loss: 0.301\n",
      "[Epoch 84,   100 Mini Batches] loss: 0.314\n",
      "[Epoch 84,   150 Mini Batches] loss: 0.331\n",
      "tensor(0.3518)\n",
      "[Epoch 85,    50 Mini Batches] loss: 0.304\n",
      "[Epoch 85,   100 Mini Batches] loss: 0.308\n",
      "[Epoch 85,   150 Mini Batches] loss: 0.319\n",
      "tensor(0.3496)\n",
      "[Epoch 86,    50 Mini Batches] loss: 0.303\n",
      "[Epoch 86,   100 Mini Batches] loss: 0.317\n",
      "[Epoch 86,   150 Mini Batches] loss: 0.314\n",
      "tensor(0.3505)\n",
      "[Epoch 87,    50 Mini Batches] loss: 0.320\n",
      "[Epoch 87,   100 Mini Batches] loss: 0.303\n",
      "[Epoch 87,   150 Mini Batches] loss: 0.316\n",
      "tensor(0.3572)\n",
      "[Epoch 88,    50 Mini Batches] loss: 0.308\n",
      "[Epoch 88,   100 Mini Batches] loss: 0.308\n",
      "[Epoch 88,   150 Mini Batches] loss: 0.319\n",
      "tensor(0.3500)\n",
      "[Epoch 89,    50 Mini Batches] loss: 0.302\n",
      "[Epoch 89,   100 Mini Batches] loss: 0.308\n",
      "[Epoch 89,   150 Mini Batches] loss: 0.304\n",
      "tensor(0.3454)\n",
      "[Epoch 90,    50 Mini Batches] loss: 0.303\n",
      "[Epoch 90,   100 Mini Batches] loss: 0.312\n",
      "[Epoch 90,   150 Mini Batches] loss: 0.293\n",
      "tensor(0.3441)\n",
      "[Epoch 91,    50 Mini Batches] loss: 0.310\n",
      "[Epoch 91,   100 Mini Batches] loss: 0.301\n",
      "[Epoch 91,   150 Mini Batches] loss: 0.301\n",
      "tensor(0.3578)\n",
      "[Epoch 92,    50 Mini Batches] loss: 0.303\n",
      "[Epoch 92,   100 Mini Batches] loss: 0.306\n",
      "[Epoch 92,   150 Mini Batches] loss: 0.310\n",
      "tensor(0.3470)\n",
      "[Epoch 93,    50 Mini Batches] loss: 0.300\n",
      "[Epoch 93,   100 Mini Batches] loss: 0.301\n",
      "[Epoch 93,   150 Mini Batches] loss: 0.319\n",
      "tensor(0.3457)\n",
      "[Epoch 94,    50 Mini Batches] loss: 0.301\n",
      "[Epoch 94,   100 Mini Batches] loss: 0.293\n",
      "[Epoch 94,   150 Mini Batches] loss: 0.306\n",
      "tensor(0.3425)\n",
      "[Epoch 95,    50 Mini Batches] loss: 0.316\n",
      "[Epoch 95,   100 Mini Batches] loss: 0.296\n",
      "[Epoch 95,   150 Mini Batches] loss: 0.301\n",
      "tensor(0.3492)\n",
      "[Epoch 96,    50 Mini Batches] loss: 0.291\n",
      "[Epoch 96,   100 Mini Batches] loss: 0.300\n",
      "[Epoch 96,   150 Mini Batches] loss: 0.317\n",
      "tensor(0.3444)\n",
      "[Epoch 97,    50 Mini Batches] loss: 0.302\n",
      "[Epoch 97,   100 Mini Batches] loss: 0.310\n",
      "[Epoch 97,   150 Mini Batches] loss: 0.296\n",
      "tensor(0.3494)\n",
      "[Epoch 98,    50 Mini Batches] loss: 0.292\n",
      "[Epoch 98,   100 Mini Batches] loss: 0.288\n",
      "[Epoch 98,   150 Mini Batches] loss: 0.315\n",
      "tensor(0.3471)\n",
      "[Epoch 99,    50 Mini Batches] loss: 0.295\n",
      "[Epoch 99,   100 Mini Batches] loss: 0.303\n",
      "[Epoch 99,   150 Mini Batches] loss: 0.301\n",
      "tensor(0.3462)\n",
      "[Epoch 100,    50 Mini Batches] loss: 0.297\n",
      "[Epoch 100,   100 Mini Batches] loss: 0.286\n",
      "[Epoch 100,   150 Mini Batches] loss: 0.297\n",
      "tensor(0.3456)\n",
      "[Epoch 101,    50 Mini Batches] loss: 0.287\n",
      "[Epoch 101,   100 Mini Batches] loss: 0.301\n",
      "[Epoch 101,   150 Mini Batches] loss: 0.292\n",
      "tensor(0.3537)\n",
      "[Epoch 102,    50 Mini Batches] loss: 0.282\n",
      "[Epoch 102,   100 Mini Batches] loss: 0.298\n",
      "[Epoch 102,   150 Mini Batches] loss: 0.306\n",
      "tensor(0.3563)\n",
      "[Epoch 103,    50 Mini Batches] loss: 0.275\n",
      "[Epoch 103,   100 Mini Batches] loss: 0.294\n",
      "[Epoch 103,   150 Mini Batches] loss: 0.305\n",
      "tensor(0.3462)\n",
      "[Epoch 104,    50 Mini Batches] loss: 0.287\n",
      "[Epoch 104,   100 Mini Batches] loss: 0.297\n",
      "[Epoch 104,   150 Mini Batches] loss: 0.284\n",
      "tensor(0.3462)\n",
      "[Epoch 105,    50 Mini Batches] loss: 0.286\n",
      "[Epoch 105,   100 Mini Batches] loss: 0.302\n",
      "[Epoch 105,   150 Mini Batches] loss: 0.288\n",
      "tensor(0.3450)\n",
      "[Epoch 106,    50 Mini Batches] loss: 0.286\n",
      "[Epoch 106,   100 Mini Batches] loss: 0.287\n",
      "[Epoch 106,   150 Mini Batches] loss: 0.294\n",
      "tensor(0.3457)\n",
      "[Epoch 107,    50 Mini Batches] loss: 0.281\n",
      "[Epoch 107,   100 Mini Batches] loss: 0.284\n",
      "[Epoch 107,   150 Mini Batches] loss: 0.281\n",
      "tensor(0.3480)\n",
      "[Epoch 108,    50 Mini Batches] loss: 0.276\n",
      "[Epoch 108,   100 Mini Batches] loss: 0.290\n",
      "[Epoch 108,   150 Mini Batches] loss: 0.293\n",
      "tensor(0.3435)\n",
      "[Epoch 109,    50 Mini Batches] loss: 0.297\n",
      "[Epoch 109,   100 Mini Batches] loss: 0.281\n",
      "[Epoch 109,   150 Mini Batches] loss: 0.292\n",
      "tensor(0.3479)\n",
      "[Epoch 110,    50 Mini Batches] loss: 0.286\n",
      "[Epoch 110,   100 Mini Batches] loss: 0.291\n",
      "[Epoch 110,   150 Mini Batches] loss: 0.282\n",
      "tensor(0.3478)\n",
      "[Epoch 111,    50 Mini Batches] loss: 0.277\n",
      "[Epoch 111,   100 Mini Batches] loss: 0.282\n",
      "[Epoch 111,   150 Mini Batches] loss: 0.294\n",
      "tensor(0.3476)\n",
      "[Epoch 112,    50 Mini Batches] loss: 0.272\n",
      "[Epoch 112,   100 Mini Batches] loss: 0.287\n",
      "[Epoch 112,   150 Mini Batches] loss: 0.282\n",
      "tensor(0.3462)\n",
      "[Epoch 113,    50 Mini Batches] loss: 0.276\n",
      "[Epoch 113,   100 Mini Batches] loss: 0.276\n",
      "[Epoch 113,   150 Mini Batches] loss: 0.275\n",
      "tensor(0.3450)\n",
      "[Epoch 114,    50 Mini Batches] loss: 0.256\n",
      "[Epoch 114,   100 Mini Batches] loss: 0.299\n",
      "[Epoch 114,   150 Mini Batches] loss: 0.287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3532)\n",
      "[Epoch 115,    50 Mini Batches] loss: 0.279\n",
      "[Epoch 115,   100 Mini Batches] loss: 0.275\n",
      "[Epoch 115,   150 Mini Batches] loss: 0.270\n",
      "tensor(0.3502)\n",
      "[Epoch 116,    50 Mini Batches] loss: 0.287\n",
      "[Epoch 116,   100 Mini Batches] loss: 0.269\n",
      "[Epoch 116,   150 Mini Batches] loss: 0.280\n",
      "tensor(0.3539)\n",
      "[Epoch 117,    50 Mini Batches] loss: 0.270\n",
      "[Epoch 117,   100 Mini Batches] loss: 0.272\n",
      "[Epoch 117,   150 Mini Batches] loss: 0.295\n",
      "tensor(0.3496)\n",
      "[Epoch 118,    50 Mini Batches] loss: 0.273\n",
      "[Epoch 118,   100 Mini Batches] loss: 0.281\n",
      "[Epoch 118,   150 Mini Batches] loss: 0.282\n",
      "tensor(0.3506)\n",
      "[Epoch 119,    50 Mini Batches] loss: 0.285\n",
      "[Epoch 119,   100 Mini Batches] loss: 0.265\n",
      "[Epoch 119,   150 Mini Batches] loss: 0.270\n",
      "tensor(0.3484)\n",
      "[Epoch 120,    50 Mini Batches] loss: 0.280\n",
      "[Epoch 120,   100 Mini Batches] loss: 0.269\n",
      "[Epoch 120,   150 Mini Batches] loss: 0.284\n",
      "tensor(0.3605)\n",
      "[Epoch 121,    50 Mini Batches] loss: 0.275\n",
      "[Epoch 121,   100 Mini Batches] loss: 0.280\n",
      "[Epoch 121,   150 Mini Batches] loss: 0.273\n",
      "tensor(0.3494)\n",
      "[Epoch 122,    50 Mini Batches] loss: 0.275\n",
      "[Epoch 122,   100 Mini Batches] loss: 0.261\n",
      "[Epoch 122,   150 Mini Batches] loss: 0.273\n",
      "tensor(0.3532)\n",
      "[Epoch 123,    50 Mini Batches] loss: 0.264\n",
      "[Epoch 123,   100 Mini Batches] loss: 0.279\n",
      "[Epoch 123,   150 Mini Batches] loss: 0.265\n",
      "tensor(0.3517)\n",
      "[Epoch 124,    50 Mini Batches] loss: 0.262\n",
      "[Epoch 124,   100 Mini Batches] loss: 0.264\n",
      "[Epoch 124,   150 Mini Batches] loss: 0.283\n",
      "tensor(0.3529)\n",
      "[Epoch 125,    50 Mini Batches] loss: 0.271\n",
      "[Epoch 125,   100 Mini Batches] loss: 0.275\n",
      "[Epoch 125,   150 Mini Batches] loss: 0.263\n",
      "tensor(0.3500)\n",
      "[Epoch 126,    50 Mini Batches] loss: 0.263\n",
      "[Epoch 126,   100 Mini Batches] loss: 0.271\n",
      "[Epoch 126,   150 Mini Batches] loss: 0.280\n",
      "tensor(0.3653)\n",
      "[Epoch 127,    50 Mini Batches] loss: 0.277\n",
      "[Epoch 127,   100 Mini Batches] loss: 0.277\n",
      "[Epoch 127,   150 Mini Batches] loss: 0.259\n",
      "tensor(0.3566)\n",
      "[Epoch 128,    50 Mini Batches] loss: 0.261\n",
      "[Epoch 128,   100 Mini Batches] loss: 0.280\n",
      "[Epoch 128,   150 Mini Batches] loss: 0.281\n",
      "tensor(0.3499)\n",
      "[Epoch 129,    50 Mini Batches] loss: 0.273\n",
      "[Epoch 129,   100 Mini Batches] loss: 0.265\n",
      "[Epoch 129,   150 Mini Batches] loss: 0.273\n",
      "tensor(0.3495)\n",
      "[Epoch 130,    50 Mini Batches] loss: 0.250\n",
      "[Epoch 130,   100 Mini Batches] loss: 0.272\n",
      "[Epoch 130,   150 Mini Batches] loss: 0.270\n",
      "tensor(0.3572)\n",
      "[Epoch 131,    50 Mini Batches] loss: 0.274\n",
      "[Epoch 131,   100 Mini Batches] loss: 0.266\n",
      "[Epoch 131,   150 Mini Batches] loss: 0.261\n",
      "tensor(0.3556)\n",
      "[Epoch 132,    50 Mini Batches] loss: 0.259\n",
      "[Epoch 132,   100 Mini Batches] loss: 0.271\n",
      "[Epoch 132,   150 Mini Batches] loss: 0.256\n",
      "tensor(0.3518)\n",
      "[Epoch 133,    50 Mini Batches] loss: 0.254\n",
      "[Epoch 133,   100 Mini Batches] loss: 0.271\n",
      "[Epoch 133,   150 Mini Batches] loss: 0.267\n",
      "tensor(0.3523)\n",
      "[Epoch 134,    50 Mini Batches] loss: 0.257\n",
      "[Epoch 134,   100 Mini Batches] loss: 0.276\n",
      "[Epoch 134,   150 Mini Batches] loss: 0.257\n",
      "tensor(0.3537)\n",
      "[Epoch 135,    50 Mini Batches] loss: 0.261\n",
      "[Epoch 135,   100 Mini Batches] loss: 0.263\n",
      "[Epoch 135,   150 Mini Batches] loss: 0.271\n",
      "tensor(0.3576)\n",
      "[Epoch 136,    50 Mini Batches] loss: 0.254\n",
      "[Epoch 136,   100 Mini Batches] loss: 0.271\n",
      "[Epoch 136,   150 Mini Batches] loss: 0.273\n",
      "tensor(0.3518)\n",
      "[Epoch 137,    50 Mini Batches] loss: 0.264\n",
      "[Epoch 137,   100 Mini Batches] loss: 0.246\n",
      "[Epoch 137,   150 Mini Batches] loss: 0.265\n",
      "tensor(0.3706)\n",
      "[Epoch 138,    50 Mini Batches] loss: 0.248\n",
      "[Epoch 138,   100 Mini Batches] loss: 0.268\n",
      "[Epoch 138,   150 Mini Batches] loss: 0.254\n",
      "tensor(0.3521)\n",
      "[Epoch 139,    50 Mini Batches] loss: 0.262\n",
      "[Epoch 139,   100 Mini Batches] loss: 0.259\n",
      "[Epoch 139,   150 Mini Batches] loss: 0.258\n",
      "tensor(0.3554)\n",
      "[Epoch 140,    50 Mini Batches] loss: 0.239\n",
      "[Epoch 140,   100 Mini Batches] loss: 0.269\n",
      "[Epoch 140,   150 Mini Batches] loss: 0.252\n",
      "tensor(0.3592)\n",
      "[Epoch 141,    50 Mini Batches] loss: 0.250\n",
      "[Epoch 141,   100 Mini Batches] loss: 0.256\n",
      "[Epoch 141,   150 Mini Batches] loss: 0.259\n",
      "tensor(0.3546)\n",
      "[Epoch 142,    50 Mini Batches] loss: 0.238\n",
      "[Epoch 142,   100 Mini Batches] loss: 0.264\n",
      "[Epoch 142,   150 Mini Batches] loss: 0.261\n",
      "tensor(0.3527)\n",
      "[Epoch 143,    50 Mini Batches] loss: 0.254\n",
      "[Epoch 143,   100 Mini Batches] loss: 0.260\n",
      "[Epoch 143,   150 Mini Batches] loss: 0.254\n",
      "tensor(0.3573)\n",
      "[Epoch 144,    50 Mini Batches] loss: 0.264\n",
      "[Epoch 144,   100 Mini Batches] loss: 0.234\n",
      "[Epoch 144,   150 Mini Batches] loss: 0.251\n",
      "tensor(0.3691)\n",
      "[Epoch 145,    50 Mini Batches] loss: 0.258\n",
      "[Epoch 145,   100 Mini Batches] loss: 0.257\n",
      "[Epoch 145,   150 Mini Batches] loss: 0.255\n",
      "tensor(0.3564)\n",
      "[Epoch 146,    50 Mini Batches] loss: 0.252\n",
      "[Epoch 146,   100 Mini Batches] loss: 0.239\n",
      "[Epoch 146,   150 Mini Batches] loss: 0.257\n",
      "tensor(0.3632)\n",
      "[Epoch 147,    50 Mini Batches] loss: 0.247\n",
      "[Epoch 147,   100 Mini Batches] loss: 0.261\n",
      "[Epoch 147,   150 Mini Batches] loss: 0.246\n",
      "tensor(0.3601)\n",
      "[Epoch 148,    50 Mini Batches] loss: 0.245\n",
      "[Epoch 148,   100 Mini Batches] loss: 0.258\n",
      "[Epoch 148,   150 Mini Batches] loss: 0.255\n",
      "tensor(0.3521)\n",
      "[Epoch 149,    50 Mini Batches] loss: 0.266\n",
      "[Epoch 149,   100 Mini Batches] loss: 0.236\n",
      "[Epoch 149,   150 Mini Batches] loss: 0.261\n",
      "tensor(0.3578)\n",
      "[Epoch 150,    50 Mini Batches] loss: 0.250\n",
      "[Epoch 150,   100 Mini Batches] loss: 0.261\n",
      "[Epoch 150,   150 Mini Batches] loss: 0.251\n",
      "tensor(0.3579)\n",
      "[Epoch 151,    50 Mini Batches] loss: 0.241\n",
      "[Epoch 151,   100 Mini Batches] loss: 0.248\n",
      "[Epoch 151,   150 Mini Batches] loss: 0.253\n",
      "tensor(0.3594)\n",
      "[Epoch 152,    50 Mini Batches] loss: 0.248\n",
      "[Epoch 152,   100 Mini Batches] loss: 0.260\n",
      "[Epoch 152,   150 Mini Batches] loss: 0.259\n",
      "tensor(0.3595)\n",
      "[Epoch 153,    50 Mini Batches] loss: 0.248\n",
      "[Epoch 153,   100 Mini Batches] loss: 0.254\n",
      "[Epoch 153,   150 Mini Batches] loss: 0.239\n",
      "tensor(0.3700)\n",
      "[Epoch 154,    50 Mini Batches] loss: 0.246\n",
      "[Epoch 154,   100 Mini Batches] loss: 0.250\n",
      "[Epoch 154,   150 Mini Batches] loss: 0.248\n",
      "tensor(0.3629)\n",
      "[Epoch 155,    50 Mini Batches] loss: 0.249\n",
      "[Epoch 155,   100 Mini Batches] loss: 0.249\n",
      "[Epoch 155,   150 Mini Batches] loss: 0.235\n",
      "tensor(0.3639)\n",
      "[Epoch 156,    50 Mini Batches] loss: 0.229\n",
      "[Epoch 156,   100 Mini Batches] loss: 0.238\n",
      "[Epoch 156,   150 Mini Batches] loss: 0.258\n",
      "tensor(0.3604)\n",
      "[Epoch 157,    50 Mini Batches] loss: 0.243\n",
      "[Epoch 157,   100 Mini Batches] loss: 0.249\n",
      "[Epoch 157,   150 Mini Batches] loss: 0.242\n",
      "tensor(0.3588)\n",
      "[Epoch 158,    50 Mini Batches] loss: 0.234\n",
      "[Epoch 158,   100 Mini Batches] loss: 0.233\n",
      "[Epoch 158,   150 Mini Batches] loss: 0.259\n",
      "tensor(0.3579)\n",
      "[Epoch 159,    50 Mini Batches] loss: 0.241\n",
      "[Epoch 159,   100 Mini Batches] loss: 0.223\n",
      "[Epoch 159,   150 Mini Batches] loss: 0.247\n",
      "tensor(0.3636)\n",
      "[Epoch 160,    50 Mini Batches] loss: 0.238\n",
      "[Epoch 160,   100 Mini Batches] loss: 0.247\n",
      "[Epoch 160,   150 Mini Batches] loss: 0.260\n",
      "tensor(0.3583)\n",
      "[Epoch 161,    50 Mini Batches] loss: 0.235\n",
      "[Epoch 161,   100 Mini Batches] loss: 0.250\n",
      "[Epoch 161,   150 Mini Batches] loss: 0.238\n",
      "tensor(0.3583)\n",
      "[Epoch 162,    50 Mini Batches] loss: 0.234\n",
      "[Epoch 162,   100 Mini Batches] loss: 0.241\n",
      "[Epoch 162,   150 Mini Batches] loss: 0.235\n",
      "tensor(0.3629)\n",
      "[Epoch 163,    50 Mini Batches] loss: 0.238\n",
      "[Epoch 163,   100 Mini Batches] loss: 0.243\n",
      "[Epoch 163,   150 Mini Batches] loss: 0.237\n",
      "tensor(0.3597)\n",
      "[Epoch 164,    50 Mini Batches] loss: 0.247\n",
      "[Epoch 164,   100 Mini Batches] loss: 0.239\n",
      "[Epoch 164,   150 Mini Batches] loss: 0.235\n",
      "tensor(0.3645)\n",
      "[Epoch 165,    50 Mini Batches] loss: 0.231\n",
      "[Epoch 165,   100 Mini Batches] loss: 0.249\n",
      "[Epoch 165,   150 Mini Batches] loss: 0.236\n",
      "tensor(0.3600)\n",
      "[Epoch 166,    50 Mini Batches] loss: 0.230\n",
      "[Epoch 166,   100 Mini Batches] loss: 0.236\n",
      "[Epoch 166,   150 Mini Batches] loss: 0.242\n",
      "tensor(0.3579)\n",
      "[Epoch 167,    50 Mini Batches] loss: 0.230\n",
      "[Epoch 167,   100 Mini Batches] loss: 0.246\n",
      "[Epoch 167,   150 Mini Batches] loss: 0.236\n",
      "tensor(0.3647)\n",
      "[Epoch 168,    50 Mini Batches] loss: 0.228\n",
      "[Epoch 168,   100 Mini Batches] loss: 0.238\n",
      "[Epoch 168,   150 Mini Batches] loss: 0.240\n",
      "tensor(0.3610)\n",
      "[Epoch 169,    50 Mini Batches] loss: 0.228\n",
      "[Epoch 169,   100 Mini Batches] loss: 0.244\n",
      "[Epoch 169,   150 Mini Batches] loss: 0.251\n",
      "tensor(0.3608)\n",
      "[Epoch 170,    50 Mini Batches] loss: 0.232\n",
      "[Epoch 170,   100 Mini Batches] loss: 0.228\n",
      "[Epoch 170,   150 Mini Batches] loss: 0.243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3705)\n",
      "[Epoch 171,    50 Mini Batches] loss: 0.239\n",
      "[Epoch 171,   100 Mini Batches] loss: 0.235\n",
      "[Epoch 171,   150 Mini Batches] loss: 0.237\n",
      "tensor(0.3672)\n",
      "[Epoch 172,    50 Mini Batches] loss: 0.238\n",
      "[Epoch 172,   100 Mini Batches] loss: 0.216\n",
      "[Epoch 172,   150 Mini Batches] loss: 0.240\n",
      "tensor(0.3669)\n",
      "[Epoch 173,    50 Mini Batches] loss: 0.230\n",
      "[Epoch 173,   100 Mini Batches] loss: 0.223\n",
      "[Epoch 173,   150 Mini Batches] loss: 0.242\n",
      "tensor(0.3690)\n",
      "[Epoch 174,    50 Mini Batches] loss: 0.232\n",
      "[Epoch 174,   100 Mini Batches] loss: 0.245\n",
      "[Epoch 174,   150 Mini Batches] loss: 0.229\n",
      "tensor(0.3610)\n",
      "[Epoch 175,    50 Mini Batches] loss: 0.233\n",
      "[Epoch 175,   100 Mini Batches] loss: 0.231\n",
      "[Epoch 175,   150 Mini Batches] loss: 0.222\n",
      "tensor(0.3665)\n",
      "[Epoch 176,    50 Mini Batches] loss: 0.223\n",
      "[Epoch 176,   100 Mini Batches] loss: 0.231\n",
      "[Epoch 176,   150 Mini Batches] loss: 0.228\n",
      "tensor(0.3721)\n",
      "[Epoch 177,    50 Mini Batches] loss: 0.227\n",
      "[Epoch 177,   100 Mini Batches] loss: 0.239\n",
      "[Epoch 177,   150 Mini Batches] loss: 0.230\n",
      "tensor(0.3687)\n",
      "[Epoch 178,    50 Mini Batches] loss: 0.245\n",
      "[Epoch 178,   100 Mini Batches] loss: 0.224\n",
      "[Epoch 178,   150 Mini Batches] loss: 0.230\n",
      "tensor(0.3693)\n",
      "[Epoch 179,    50 Mini Batches] loss: 0.218\n",
      "[Epoch 179,   100 Mini Batches] loss: 0.232\n",
      "[Epoch 179,   150 Mini Batches] loss: 0.229\n",
      "tensor(0.3722)\n",
      "[Epoch 180,    50 Mini Batches] loss: 0.226\n",
      "[Epoch 180,   100 Mini Batches] loss: 0.230\n",
      "[Epoch 180,   150 Mini Batches] loss: 0.235\n",
      "tensor(0.3808)\n",
      "[Epoch 181,    50 Mini Batches] loss: 0.222\n",
      "[Epoch 181,   100 Mini Batches] loss: 0.236\n",
      "[Epoch 181,   150 Mini Batches] loss: 0.229\n",
      "tensor(0.3705)\n",
      "[Epoch 182,    50 Mini Batches] loss: 0.225\n",
      "[Epoch 182,   100 Mini Batches] loss: 0.223\n",
      "[Epoch 182,   150 Mini Batches] loss: 0.235\n",
      "tensor(0.3759)\n",
      "[Epoch 183,    50 Mini Batches] loss: 0.217\n",
      "[Epoch 183,   100 Mini Batches] loss: 0.235\n",
      "[Epoch 183,   150 Mini Batches] loss: 0.242\n",
      "tensor(0.3664)\n",
      "[Epoch 184,    50 Mini Batches] loss: 0.218\n",
      "[Epoch 184,   100 Mini Batches] loss: 0.231\n",
      "[Epoch 184,   150 Mini Batches] loss: 0.226\n",
      "tensor(0.3735)\n",
      "[Epoch 185,    50 Mini Batches] loss: 0.221\n",
      "[Epoch 185,   100 Mini Batches] loss: 0.228\n",
      "[Epoch 185,   150 Mini Batches] loss: 0.228\n",
      "tensor(0.3656)\n",
      "[Epoch 186,    50 Mini Batches] loss: 0.240\n",
      "[Epoch 186,   100 Mini Batches] loss: 0.218\n",
      "[Epoch 186,   150 Mini Batches] loss: 0.221\n",
      "tensor(0.3693)\n",
      "[Epoch 187,    50 Mini Batches] loss: 0.215\n",
      "[Epoch 187,   100 Mini Batches] loss: 0.234\n",
      "[Epoch 187,   150 Mini Batches] loss: 0.213\n",
      "tensor(0.3721)\n",
      "[Epoch 188,    50 Mini Batches] loss: 0.215\n",
      "[Epoch 188,   100 Mini Batches] loss: 0.229\n",
      "[Epoch 188,   150 Mini Batches] loss: 0.236\n",
      "tensor(0.3752)\n",
      "[Epoch 189,    50 Mini Batches] loss: 0.223\n",
      "[Epoch 189,   100 Mini Batches] loss: 0.210\n",
      "[Epoch 189,   150 Mini Batches] loss: 0.222\n",
      "tensor(0.3680)\n",
      "[Epoch 190,    50 Mini Batches] loss: 0.233\n",
      "[Epoch 190,   100 Mini Batches] loss: 0.215\n",
      "[Epoch 190,   150 Mini Batches] loss: 0.237\n",
      "tensor(0.3682)\n",
      "[Epoch 191,    50 Mini Batches] loss: 0.226\n",
      "[Epoch 191,   100 Mini Batches] loss: 0.224\n",
      "[Epoch 191,   150 Mini Batches] loss: 0.211\n",
      "tensor(0.3730)\n",
      "[Epoch 192,    50 Mini Batches] loss: 0.218\n",
      "[Epoch 192,   100 Mini Batches] loss: 0.222\n",
      "[Epoch 192,   150 Mini Batches] loss: 0.231\n",
      "tensor(0.3716)\n",
      "[Epoch 193,    50 Mini Batches] loss: 0.215\n",
      "[Epoch 193,   100 Mini Batches] loss: 0.221\n",
      "[Epoch 193,   150 Mini Batches] loss: 0.209\n",
      "tensor(0.3709)\n",
      "[Epoch 194,    50 Mini Batches] loss: 0.226\n",
      "[Epoch 194,   100 Mini Batches] loss: 0.224\n",
      "[Epoch 194,   150 Mini Batches] loss: 0.221\n",
      "tensor(0.3725)\n",
      "[Epoch 195,    50 Mini Batches] loss: 0.220\n",
      "[Epoch 195,   100 Mini Batches] loss: 0.209\n",
      "[Epoch 195,   150 Mini Batches] loss: 0.219\n",
      "tensor(0.3894)\n",
      "[Epoch 196,    50 Mini Batches] loss: 0.202\n",
      "[Epoch 196,   100 Mini Batches] loss: 0.216\n",
      "[Epoch 196,   150 Mini Batches] loss: 0.224\n",
      "tensor(0.3828)\n",
      "[Epoch 197,    50 Mini Batches] loss: 0.224\n",
      "[Epoch 197,   100 Mini Batches] loss: 0.221\n",
      "[Epoch 197,   150 Mini Batches] loss: 0.226\n",
      "tensor(0.3783)\n",
      "[Epoch 198,    50 Mini Batches] loss: 0.227\n",
      "[Epoch 198,   100 Mini Batches] loss: 0.212\n",
      "[Epoch 198,   150 Mini Batches] loss: 0.215\n",
      "tensor(0.3807)\n",
      "[Epoch 199,    50 Mini Batches] loss: 0.219\n",
      "[Epoch 199,   100 Mini Batches] loss: 0.227\n",
      "[Epoch 199,   150 Mini Batches] loss: 0.214\n",
      "tensor(0.3755)\n",
      "[Epoch 200,    50 Mini Batches] loss: 0.219\n",
      "[Epoch 200,   100 Mini Batches] loss: 0.210\n",
      "[Epoch 200,   150 Mini Batches] loss: 0.213\n",
      "tensor(0.3766)\n",
      "[Epoch 201,    50 Mini Batches] loss: 0.217\n",
      "[Epoch 201,   100 Mini Batches] loss: 0.219\n",
      "[Epoch 201,   150 Mini Batches] loss: 0.216\n",
      "tensor(0.3745)\n",
      "[Epoch 202,    50 Mini Batches] loss: 0.193\n",
      "[Epoch 202,   100 Mini Batches] loss: 0.213\n",
      "[Epoch 202,   150 Mini Batches] loss: 0.225\n",
      "tensor(0.3764)\n",
      "[Epoch 203,    50 Mini Batches] loss: 0.220\n",
      "[Epoch 203,   100 Mini Batches] loss: 0.231\n",
      "[Epoch 203,   150 Mini Batches] loss: 0.203\n",
      "tensor(0.3786)\n",
      "[Epoch 204,    50 Mini Batches] loss: 0.217\n",
      "[Epoch 204,   100 Mini Batches] loss: 0.218\n",
      "[Epoch 204,   150 Mini Batches] loss: 0.210\n",
      "tensor(0.3758)\n",
      "[Epoch 205,    50 Mini Batches] loss: 0.198\n",
      "[Epoch 205,   100 Mini Batches] loss: 0.225\n",
      "[Epoch 205,   150 Mini Batches] loss: 0.213\n",
      "tensor(0.3705)\n",
      "[Epoch 206,    50 Mini Batches] loss: 0.202\n",
      "[Epoch 206,   100 Mini Batches] loss: 0.224\n",
      "[Epoch 206,   150 Mini Batches] loss: 0.223\n",
      "tensor(0.3737)\n",
      "[Epoch 207,    50 Mini Batches] loss: 0.210\n",
      "[Epoch 207,   100 Mini Batches] loss: 0.216\n",
      "[Epoch 207,   150 Mini Batches] loss: 0.227\n",
      "tensor(0.3813)\n",
      "[Epoch 208,    50 Mini Batches] loss: 0.217\n",
      "[Epoch 208,   100 Mini Batches] loss: 0.214\n",
      "[Epoch 208,   150 Mini Batches] loss: 0.212\n",
      "tensor(0.3937)\n",
      "[Epoch 209,    50 Mini Batches] loss: 0.202\n",
      "[Epoch 209,   100 Mini Batches] loss: 0.231\n",
      "[Epoch 209,   150 Mini Batches] loss: 0.210\n",
      "tensor(0.3787)\n",
      "[Epoch 210,    50 Mini Batches] loss: 0.202\n",
      "[Epoch 210,   100 Mini Batches] loss: 0.207\n",
      "[Epoch 210,   150 Mini Batches] loss: 0.213\n",
      "tensor(0.3699)\n",
      "[Epoch 211,    50 Mini Batches] loss: 0.216\n",
      "[Epoch 211,   100 Mini Batches] loss: 0.213\n",
      "[Epoch 211,   150 Mini Batches] loss: 0.209\n",
      "tensor(0.3832)\n",
      "[Epoch 212,    50 Mini Batches] loss: 0.224\n",
      "[Epoch 212,   100 Mini Batches] loss: 0.210\n",
      "[Epoch 212,   150 Mini Batches] loss: 0.203\n",
      "tensor(0.3818)\n",
      "[Epoch 213,    50 Mini Batches] loss: 0.206\n",
      "[Epoch 213,   100 Mini Batches] loss: 0.209\n",
      "[Epoch 213,   150 Mini Batches] loss: 0.205\n",
      "tensor(0.3804)\n",
      "[Epoch 214,    50 Mini Batches] loss: 0.211\n",
      "[Epoch 214,   100 Mini Batches] loss: 0.203\n",
      "[Epoch 214,   150 Mini Batches] loss: 0.213\n",
      "tensor(0.3758)\n",
      "[Epoch 215,    50 Mini Batches] loss: 0.209\n",
      "[Epoch 215,   100 Mini Batches] loss: 0.198\n",
      "[Epoch 215,   150 Mini Batches] loss: 0.217\n",
      "tensor(0.3864)\n",
      "[Epoch 216,    50 Mini Batches] loss: 0.208\n",
      "[Epoch 216,   100 Mini Batches] loss: 0.215\n",
      "[Epoch 216,   150 Mini Batches] loss: 0.213\n",
      "tensor(0.3898)\n",
      "[Epoch 217,    50 Mini Batches] loss: 0.213\n",
      "[Epoch 217,   100 Mini Batches] loss: 0.193\n",
      "[Epoch 217,   150 Mini Batches] loss: 0.220\n",
      "tensor(0.3867)\n",
      "[Epoch 218,    50 Mini Batches] loss: 0.202\n",
      "[Epoch 218,   100 Mini Batches] loss: 0.205\n",
      "[Epoch 218,   150 Mini Batches] loss: 0.213\n",
      "tensor(0.3832)\n",
      "[Epoch 219,    50 Mini Batches] loss: 0.203\n",
      "[Epoch 219,   100 Mini Batches] loss: 0.194\n",
      "[Epoch 219,   150 Mini Batches] loss: 0.208\n",
      "tensor(0.3779)\n",
      "[Epoch 220,    50 Mini Batches] loss: 0.200\n",
      "[Epoch 220,   100 Mini Batches] loss: 0.210\n",
      "[Epoch 220,   150 Mini Batches] loss: 0.206\n",
      "tensor(0.3801)\n",
      "[Epoch 221,    50 Mini Batches] loss: 0.192\n",
      "[Epoch 221,   100 Mini Batches] loss: 0.207\n",
      "[Epoch 221,   150 Mini Batches] loss: 0.204\n",
      "tensor(0.3797)\n",
      "[Epoch 222,    50 Mini Batches] loss: 0.208\n",
      "[Epoch 222,   100 Mini Batches] loss: 0.211\n",
      "[Epoch 222,   150 Mini Batches] loss: 0.210\n",
      "tensor(0.3874)\n",
      "[Epoch 223,    50 Mini Batches] loss: 0.206\n",
      "[Epoch 223,   100 Mini Batches] loss: 0.203\n",
      "[Epoch 223,   150 Mini Batches] loss: 0.208\n",
      "tensor(0.3830)\n",
      "[Epoch 224,    50 Mini Batches] loss: 0.208\n",
      "[Epoch 224,   100 Mini Batches] loss: 0.185\n",
      "[Epoch 224,   150 Mini Batches] loss: 0.207\n",
      "tensor(0.3818)\n",
      "[Epoch 225,    50 Mini Batches] loss: 0.196\n",
      "[Epoch 225,   100 Mini Batches] loss: 0.193\n",
      "[Epoch 225,   150 Mini Batches] loss: 0.209\n",
      "tensor(0.3838)\n",
      "[Epoch 226,    50 Mini Batches] loss: 0.206\n",
      "[Epoch 226,   100 Mini Batches] loss: 0.209\n",
      "[Epoch 226,   150 Mini Batches] loss: 0.202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3848)\n",
      "[Epoch 227,    50 Mini Batches] loss: 0.204\n",
      "[Epoch 227,   100 Mini Batches] loss: 0.206\n",
      "[Epoch 227,   150 Mini Batches] loss: 0.204\n",
      "tensor(0.3861)\n",
      "[Epoch 228,    50 Mini Batches] loss: 0.202\n",
      "[Epoch 228,   100 Mini Batches] loss: 0.213\n",
      "[Epoch 228,   150 Mini Batches] loss: 0.201\n",
      "tensor(0.3901)\n",
      "[Epoch 229,    50 Mini Batches] loss: 0.198\n",
      "[Epoch 229,   100 Mini Batches] loss: 0.195\n",
      "[Epoch 229,   150 Mini Batches] loss: 0.206\n",
      "tensor(0.3899)\n",
      "[Epoch 230,    50 Mini Batches] loss: 0.192\n",
      "[Epoch 230,   100 Mini Batches] loss: 0.205\n",
      "[Epoch 230,   150 Mini Batches] loss: 0.199\n",
      "tensor(0.3854)\n",
      "[Epoch 231,    50 Mini Batches] loss: 0.193\n",
      "[Epoch 231,   100 Mini Batches] loss: 0.198\n"
     ]
    }
   ],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8594b184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test model against training data\n",
    "\n",
    "model = torch.load(\"ClassifyNet.pth\")\n",
    "\n",
    "test_score = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(test_loader):\n",
    "        image, label = data[0].float().to(device), data[1].float()\n",
    "\n",
    "        classify_output = model(image)\n",
    "\n",
    "        y_true = torch.reshape(label.cpu(), (-1,)) > 0.5\n",
    "        y_pred = torch.reshape(classify_output.cpu(), (-1,)) > 0.5\n",
    "        \n",
    "        test_loss = BACC(y_pred.numpy(), y_true.numpy())\n",
    "\n",
    "        test_score += test_loss\n",
    "\n",
    "test_score /= len(test_loader)\n",
    "\n",
    "print(test_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
